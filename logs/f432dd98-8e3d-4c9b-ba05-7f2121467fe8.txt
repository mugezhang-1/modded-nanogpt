import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = True
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20250927+cu126 compiled for CUDA 12.6
Sun Sep 28 08:52:13 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.12              Driver Version: 550.90.12      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100                    On  |   00000000:4C:00.0 Off |                    0 |
| N/A   34C    P0            114W /  700W |    3319MiB /  95830MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100                    On  |   00000000:5D:00.0 Off |                    0 |
| N/A   35C    P0            115W /  700W |    1477MiB /  95830MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100                    On  |   00000000:CC:00.0 Off |                    0 |
| N/A   35C    P0            111W /  700W |    1477MiB /  95830MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100                    On  |   00000000:DC:00.0 Off |                    0 |
| N/A   35C    P0            115W /  700W |    1477MiB /  95830MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   1101529      C   ...envs/modded-nanogpt-test/bin/python       1466MiB |
|    0   N/A  N/A   1101530      C   ...envs/modded-nanogpt-test/bin/python        608MiB |
|    0   N/A  N/A   1101531      C   ...envs/modded-nanogpt-test/bin/python        608MiB |
|    0   N/A  N/A   1101532      C   ...envs/modded-nanogpt-test/bin/python        608MiB |
|    1   N/A  N/A   1101530      C   ...envs/modded-nanogpt-test/bin/python       1466MiB |
|    2   N/A  N/A   1101531      C   ...envs/modded-nanogpt-test/bin/python       1466MiB |
|    3   N/A  N/A   1101532      C   ...envs/modded-nanogpt-test/bin/python       1466MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1750 train_time:210ms step_avg:209.96ms
step:2/1750 train_time:259ms step_avg:129.37ms
step:3/1750 train_time:400ms step_avg:133.18ms
step:4/1750 train_time:543ms step_avg:135.87ms
step:5/1750 train_time:684ms step_avg:136.85ms
step:6/1750 train_time:797ms step_avg:132.90ms
step:7/1750 train_time:910ms step_avg:129.94ms
step:8/1750 train_time:1022ms step_avg:127.70ms
step:9/1750 train_time:1152ms step_avg:127.96ms
step:10/1750 train_time:1260ms step_avg:126.02ms
step:11/1750 train_time:1369ms step_avg:124.42ms
step:12/1750 train_time:1474ms step_avg:122.80ms
step:13/1750 train_time:1586ms step_avg:121.98ms
step:14/1750 train_time:1702ms step_avg:121.55ms
step:15/1750 train_time:1815ms step_avg:120.99ms
step:16/1750 train_time:1944ms step_avg:121.51ms
step:17/1750 train_time:2103ms step_avg:123.68ms
step:18/1750 train_time:2271ms step_avg:126.19ms
step:19/1750 train_time:2380ms step_avg:125.28ms
step:20/1750 train_time:2485ms step_avg:124.27ms
step:21/1750 train_time:2590ms step_avg:123.36ms
step:22/1750 train_time:2696ms step_avg:122.54ms
step:23/1750 train_time:2801ms step_avg:121.78ms
step:24/1750 train_time:2906ms step_avg:121.07ms
step:25/1750 train_time:3014ms step_avg:120.58ms
step:26/1750 train_time:3137ms step_avg:120.64ms
step:27/1750 train_time:3221ms step_avg:119.29ms
step:28/1750 train_time:3329ms step_avg:118.91ms
step:29/1750 train_time:3431ms step_avg:118.31ms
step:30/1750 train_time:3548ms step_avg:118.26ms
step:31/1750 train_time:3653ms step_avg:117.84ms
step:32/1750 train_time:3760ms step_avg:117.51ms
step:33/1750 train_time:3868ms step_avg:117.23ms
step:34/1750 train_time:3974ms step_avg:116.88ms
step:35/1750 train_time:4081ms step_avg:116.59ms
step:36/1750 train_time:4186ms step_avg:116.27ms
step:37/1750 train_time:4292ms step_avg:115.99ms
step:38/1750 train_time:4396ms step_avg:115.69ms
step:39/1750 train_time:4501ms step_avg:115.42ms
step:40/1750 train_time:4610ms step_avg:115.25ms
step:41/1750 train_time:4711ms step_avg:114.91ms
step:42/1750 train_time:4816ms step_avg:114.67ms
step:43/1750 train_time:4921ms step_avg:114.45ms
step:44/1750 train_time:5026ms step_avg:114.24ms
step:45/1750 train_time:5131ms step_avg:114.03ms
step:46/1750 train_time:5237ms step_avg:113.84ms
step:47/1750 train_time:5342ms step_avg:113.65ms
step:48/1750 train_time:5446ms step_avg:113.47ms
step:49/1750 train_time:5551ms step_avg:113.29ms
step:50/1750 train_time:5656ms step_avg:113.13ms
step:51/1750 train_time:5761ms step_avg:112.97ms
step:52/1750 train_time:5867ms step_avg:112.82ms
step:53/1750 train_time:5972ms step_avg:112.67ms
step:54/1750 train_time:6077ms step_avg:112.54ms
step:55/1750 train_time:6182ms step_avg:112.40ms
step:56/1750 train_time:6287ms step_avg:112.27ms
step:57/1750 train_time:6392ms step_avg:112.14ms
step:58/1750 train_time:6497ms step_avg:112.02ms
step:59/1750 train_time:6602ms step_avg:111.90ms
step:60/1750 train_time:6707ms step_avg:111.78ms
step:61/1750 train_time:6812ms step_avg:111.67ms
step:62/1750 train_time:6917ms step_avg:111.56ms
step:63/1750 train_time:7022ms step_avg:111.46ms
step:64/1750 train_time:7127ms step_avg:111.36ms
step:65/1750 train_time:7232ms step_avg:111.26ms
step:66/1750 train_time:7344ms step_avg:111.27ms
step:67/1750 train_time:7442ms step_avg:111.07ms
step:68/1750 train_time:7547ms step_avg:110.99ms
step:69/1750 train_time:7652ms step_avg:110.90ms
step:70/1750 train_time:7757ms step_avg:110.82ms
step:71/1750 train_time:7862ms step_avg:110.74ms
step:72/1750 train_time:7967ms step_avg:110.65ms
step:73/1750 train_time:8072ms step_avg:110.58ms
step:74/1750 train_time:8177ms step_avg:110.50ms
step:75/1750 train_time:8282ms step_avg:110.43ms
step:76/1750 train_time:8387ms step_avg:110.36ms
step:77/1750 train_time:8492ms step_avg:110.29ms
step:78/1750 train_time:8597ms step_avg:110.22ms
step:79/1750 train_time:8702ms step_avg:110.15ms
step:80/1750 train_time:8807ms step_avg:110.09ms
step:81/1750 train_time:8922ms step_avg:110.15ms
step:82/1750 train_time:9017ms step_avg:109.97ms
step:83/1750 train_time:9129ms step_avg:109.99ms
step:84/1750 train_time:9241ms step_avg:110.01ms
step:85/1750 train_time:9353ms step_avg:110.03ms
step:86/1750 train_time:9465ms step_avg:110.05ms
step:87/1750 train_time:9576ms step_avg:110.07ms
step:88/1750 train_time:9681ms step_avg:110.01ms
step:89/1750 train_time:9787ms step_avg:109.96ms
step:90/1750 train_time:9891ms step_avg:109.90ms
step:91/1750 train_time:9996ms step_avg:109.85ms
step:92/1750 train_time:10101ms step_avg:109.79ms
step:93/1750 train_time:10206ms step_avg:109.74ms
step:94/1750 train_time:10311ms step_avg:109.69ms
step:95/1750 train_time:10416ms step_avg:109.64ms
step:96/1750 train_time:10542ms step_avg:109.81ms
step:97/1750 train_time:10625ms step_avg:109.54ms
step:98/1750 train_time:10738ms step_avg:109.57ms
step:99/1750 train_time:10836ms step_avg:109.45ms
step:100/1750 train_time:10940ms step_avg:109.40ms
step:101/1750 train_time:11045ms step_avg:109.36ms
step:102/1750 train_time:11150ms step_avg:109.31ms
step:103/1750 train_time:11255ms step_avg:109.27ms
step:104/1750 train_time:11360ms step_avg:109.23ms
step:105/1750 train_time:11468ms step_avg:109.22ms
step:106/1750 train_time:11570ms step_avg:109.15ms
step:107/1750 train_time:11675ms step_avg:109.11ms
step:108/1750 train_time:11780ms step_avg:109.07ms
step:109/1750 train_time:11885ms step_avg:109.03ms
step:110/1750 train_time:11990ms step_avg:109.00ms
step:111/1750 train_time:12122ms step_avg:109.21ms
step:112/1750 train_time:12223ms step_avg:109.13ms
step:113/1750 train_time:12305ms step_avg:108.89ms
step:114/1750 train_time:12409ms step_avg:108.85ms
step:115/1750 train_time:12514ms step_avg:108.82ms
step:116/1750 train_time:12619ms step_avg:108.79ms
step:117/1750 train_time:12724ms step_avg:108.75ms
step:118/1750 train_time:12829ms step_avg:108.72ms
step:119/1750 train_time:12933ms step_avg:108.68ms
step:120/1750 train_time:13038ms step_avg:108.65ms
step:121/1750 train_time:13143ms step_avg:108.62ms
step:122/1750 train_time:13250ms step_avg:108.60ms
step:123/1750 train_time:13352ms step_avg:108.56ms
step:124/1750 train_time:13457ms step_avg:108.53ms
step:125/1750 train_time:13562ms step_avg:108.50ms
step:125/1750 val_loss:4.6342 train_time:13663ms step_avg:109.31ms
step:126/1750 train_time:13703ms step_avg:108.75ms
step:127/1750 train_time:13835ms step_avg:108.93ms
step:128/1750 train_time:13993ms step_avg:109.32ms
step:129/1750 train_time:14142ms step_avg:109.62ms
step:130/1750 train_time:14290ms step_avg:109.92ms
step:131/1750 train_time:14439ms step_avg:110.22ms
step:132/1750 train_time:14579ms step_avg:110.45ms
step:133/1750 train_time:14691ms step_avg:110.46ms
step:134/1750 train_time:14803ms step_avg:110.47ms
step:135/1750 train_time:14923ms step_avg:110.54ms
step:136/1750 train_time:15036ms step_avg:110.56ms
step:137/1750 train_time:15148ms step_avg:110.57ms
step:138/1750 train_time:15261ms step_avg:110.59ms
step:139/1750 train_time:15380ms step_avg:110.65ms
step:140/1750 train_time:15492ms step_avg:110.66ms
step:141/1750 train_time:15604ms step_avg:110.67ms
step:142/1750 train_time:15728ms step_avg:110.76ms
step:143/1750 train_time:15848ms step_avg:110.83ms
step:144/1750 train_time:15997ms step_avg:111.09ms
step:145/1750 train_time:16105ms step_avg:111.07ms
step:146/1750 train_time:16225ms step_avg:111.13ms
step:147/1750 train_time:16337ms step_avg:111.13ms
step:148/1750 train_time:16449ms step_avg:111.14ms
step:149/1750 train_time:16561ms step_avg:111.15ms
step:150/1750 train_time:16673ms step_avg:111.15ms
step:151/1750 train_time:16786ms step_avg:111.16ms
step:152/1750 train_time:16898ms step_avg:111.17ms
step:153/1750 train_time:17014ms step_avg:111.20ms
step:154/1750 train_time:17158ms step_avg:111.42ms
step:155/1750 train_time:17271ms step_avg:111.43ms
step:156/1750 train_time:17410ms step_avg:111.60ms
step:157/1750 train_time:17543ms step_avg:111.74ms
step:158/1750 train_time:17656ms step_avg:111.74ms
step:159/1750 train_time:17768ms step_avg:111.75ms
step:160/1750 train_time:17880ms step_avg:111.75ms
step:161/1750 train_time:17992ms step_avg:111.75ms
step:162/1750 train_time:18118ms step_avg:111.84ms
step:163/1750 train_time:18264ms step_avg:112.05ms
step:164/1750 train_time:18396ms step_avg:112.17ms
step:165/1750 train_time:18529ms step_avg:112.30ms
step:166/1750 train_time:18657ms step_avg:112.39ms
step:167/1750 train_time:18785ms step_avg:112.48ms
step:168/1750 train_time:18897ms step_avg:112.48ms
step:169/1750 train_time:19006ms step_avg:112.46ms
step:170/1750 train_time:19111ms step_avg:112.42ms
step:171/1750 train_time:19216ms step_avg:112.37ms
step:172/1750 train_time:19321ms step_avg:112.33ms
step:173/1750 train_time:19426ms step_avg:112.29ms
step:174/1750 train_time:19531ms step_avg:112.25ms
step:175/1750 train_time:19637ms step_avg:112.21ms
step:176/1750 train_time:19742ms step_avg:112.17ms
step:177/1750 train_time:19848ms step_avg:112.14ms
step:178/1750 train_time:19952ms step_avg:112.09ms
step:179/1750 train_time:20057ms step_avg:112.05ms
step:180/1750 train_time:20162ms step_avg:112.01ms
step:181/1750 train_time:20267ms step_avg:111.97ms
step:182/1750 train_time:20373ms step_avg:111.94ms
step:183/1750 train_time:20478ms step_avg:111.90ms
step:184/1750 train_time:20583ms step_avg:111.86ms
step:185/1750 train_time:20688ms step_avg:111.83ms
step:186/1750 train_time:20793ms step_avg:111.79ms
step:187/1750 train_time:20898ms step_avg:111.76ms
step:188/1750 train_time:21004ms step_avg:111.72ms
step:189/1750 train_time:21109ms step_avg:111.69ms
step:190/1750 train_time:21214ms step_avg:111.65ms
step:191/1750 train_time:21319ms step_avg:111.62ms
step:192/1750 train_time:21424ms step_avg:111.58ms
step:193/1750 train_time:21529ms step_avg:111.55ms
step:194/1750 train_time:21634ms step_avg:111.52ms
step:195/1750 train_time:21743ms step_avg:111.50ms
step:196/1750 train_time:21866ms step_avg:111.56ms
step:197/1750 train_time:21950ms step_avg:111.42ms
step:198/1750 train_time:22055ms step_avg:111.39ms
step:199/1750 train_time:22160ms step_avg:111.36ms
step:200/1750 train_time:22265ms step_avg:111.33ms
step:201/1750 train_time:22370ms step_avg:111.29ms
step:202/1750 train_time:22476ms step_avg:111.27ms
step:203/1750 train_time:22581ms step_avg:111.23ms
step:204/1750 train_time:22686ms step_avg:111.21ms
step:205/1750 train_time:22791ms step_avg:111.18ms
step:206/1750 train_time:22918ms step_avg:111.25ms
step:207/1750 train_time:23001ms step_avg:111.12ms
step:208/1750 train_time:23106ms step_avg:111.09ms
step:209/1750 train_time:23211ms step_avg:111.06ms
step:210/1750 train_time:23338ms step_avg:111.13ms
step:211/1750 train_time:23443ms step_avg:111.11ms
step:212/1750 train_time:23548ms step_avg:111.08ms
step:213/1750 train_time:23653ms step_avg:111.05ms
step:214/1750 train_time:23759ms step_avg:111.02ms
step:215/1750 train_time:23864ms step_avg:110.99ms
step:216/1750 train_time:23969ms step_avg:110.97ms
step:217/1750 train_time:24081ms step_avg:110.97ms
step:218/1750 train_time:24193ms step_avg:110.98ms
step:219/1750 train_time:24305ms step_avg:110.98ms
step:220/1750 train_time:24410ms step_avg:110.96ms
step:221/1750 train_time:24523ms step_avg:110.96ms
step:222/1750 train_time:24635ms step_avg:110.97ms
step:223/1750 train_time:24770ms step_avg:111.08ms
step:224/1750 train_time:24857ms step_avg:110.97ms
step:225/1750 train_time:24961ms step_avg:110.94ms
step:226/1750 train_time:25066ms step_avg:110.91ms
step:227/1750 train_time:25171ms step_avg:110.89ms
step:228/1750 train_time:25276ms step_avg:110.86ms
step:229/1750 train_time:25382ms step_avg:110.84ms
step:230/1750 train_time:25487ms step_avg:110.81ms
step:231/1750 train_time:25592ms step_avg:110.79ms
step:232/1750 train_time:25697ms step_avg:110.76ms
step:233/1750 train_time:25803ms step_avg:110.74ms
step:234/1750 train_time:25908ms step_avg:110.72ms
step:235/1750 train_time:26013ms step_avg:110.69ms
step:236/1750 train_time:26118ms step_avg:110.67ms
step:237/1750 train_time:26225ms step_avg:110.65ms
step:238/1750 train_time:26328ms step_avg:110.62ms
step:239/1750 train_time:26433ms step_avg:110.60ms
step:240/1750 train_time:26538ms step_avg:110.58ms
step:241/1750 train_time:26643ms step_avg:110.55ms
step:242/1750 train_time:26749ms step_avg:110.53ms
step:243/1750 train_time:26854ms step_avg:110.51ms
step:244/1750 train_time:26959ms step_avg:110.49ms
step:245/1750 train_time:27064ms step_avg:110.46ms
step:246/1750 train_time:27231ms step_avg:110.70ms
step:247/1750 train_time:27368ms step_avg:110.80ms
step:248/1750 train_time:27487ms step_avg:110.83ms
step:249/1750 train_time:27622ms step_avg:110.93ms
step:250/1750 train_time:27734ms step_avg:110.94ms
step:250/1750 val_loss:4.0959 train_time:27830ms step_avg:111.32ms
step:251/1750 train_time:27883ms step_avg:111.09ms
step:252/1750 train_time:28007ms step_avg:111.14ms
step:253/1750 train_time:28115ms step_avg:111.13ms
step:254/1750 train_time:28227ms step_avg:111.13ms
step:255/1750 train_time:28326ms step_avg:111.08ms
step:256/1750 train_time:28431ms step_avg:111.06ms
step:257/1750 train_time:28536ms step_avg:111.04ms
step:258/1750 train_time:28641ms step_avg:111.01ms
step:259/1750 train_time:28759ms step_avg:111.04ms
step:260/1750 train_time:28864ms step_avg:111.02ms
step:261/1750 train_time:28980ms step_avg:111.04ms
step:262/1750 train_time:29085ms step_avg:111.01ms
step:263/1750 train_time:29197ms step_avg:111.02ms
step:264/1750 train_time:29310ms step_avg:111.02ms
step:265/1750 train_time:29422ms step_avg:111.03ms
step:266/1750 train_time:29542ms step_avg:111.06ms
step:267/1750 train_time:29677ms step_avg:111.15ms
step:268/1750 train_time:29792ms step_avg:111.16ms
step:269/1750 train_time:29922ms step_avg:111.23ms
step:270/1750 train_time:30055ms step_avg:111.32ms
step:271/1750 train_time:30174ms step_avg:111.34ms
step:272/1750 train_time:30288ms step_avg:111.35ms
step:273/1750 train_time:30401ms step_avg:111.36ms
step:274/1750 train_time:30504ms step_avg:111.33ms
step:275/1750 train_time:30609ms step_avg:111.30ms
step:276/1750 train_time:30718ms step_avg:111.30ms
step:277/1750 train_time:30846ms step_avg:111.36ms
step:278/1750 train_time:30955ms step_avg:111.35ms
step:279/1750 train_time:31064ms step_avg:111.34ms
step:280/1750 train_time:31169ms step_avg:111.32ms
step:281/1750 train_time:31274ms step_avg:111.30ms
step:282/1750 train_time:31381ms step_avg:111.28ms
step:283/1750 train_time:31486ms step_avg:111.26ms
step:284/1750 train_time:31591ms step_avg:111.24ms
step:285/1750 train_time:31721ms step_avg:111.30ms
step:286/1750 train_time:31804ms step_avg:111.20ms
step:287/1750 train_time:31909ms step_avg:111.18ms
step:288/1750 train_time:32015ms step_avg:111.16ms
step:289/1750 train_time:32123ms step_avg:111.15ms
step:290/1750 train_time:32232ms step_avg:111.15ms
step:291/1750 train_time:32338ms step_avg:111.13ms
step:292/1750 train_time:32454ms step_avg:111.14ms
step:293/1750 train_time:32563ms step_avg:111.14ms
step:294/1750 train_time:32675ms step_avg:111.14ms
step:295/1750 train_time:32784ms step_avg:111.13ms
step:296/1750 train_time:32897ms step_avg:111.14ms
step:297/1750 train_time:33009ms step_avg:111.14ms
step:298/1750 train_time:33128ms step_avg:111.17ms
step:299/1750 train_time:33249ms step_avg:111.20ms
step:300/1750 train_time:33386ms step_avg:111.29ms
step:301/1750 train_time:33513ms step_avg:111.34ms
step:302/1750 train_time:33625ms step_avg:111.34ms
step:303/1750 train_time:33734ms step_avg:111.33ms
step:304/1750 train_time:33843ms step_avg:111.33ms
step:305/1750 train_time:33952ms step_avg:111.32ms
step:306/1750 train_time:34061ms step_avg:111.31ms
step:307/1750 train_time:34170ms step_avg:111.30ms
step:308/1750 train_time:34279ms step_avg:111.30ms
step:309/1750 train_time:34391ms step_avg:111.30ms
step:310/1750 train_time:34511ms step_avg:111.33ms
step:311/1750 train_time:34627ms step_avg:111.34ms
step:312/1750 train_time:34739ms step_avg:111.34ms
step:313/1750 train_time:34852ms step_avg:111.35ms
step:314/1750 train_time:34968ms step_avg:111.36ms
step:315/1750 train_time:35081ms step_avg:111.37ms
step:316/1750 train_time:35193ms step_avg:111.37ms
step:317/1750 train_time:35298ms step_avg:111.35ms
step:318/1750 train_time:35404ms step_avg:111.33ms
step:319/1750 train_time:35509ms step_avg:111.31ms
step:320/1750 train_time:35614ms step_avg:111.29ms
step:321/1750 train_time:35720ms step_avg:111.28ms
step:322/1750 train_time:35825ms step_avg:111.26ms
step:323/1750 train_time:35967ms step_avg:111.35ms
step:324/1750 train_time:36042ms step_avg:111.24ms
step:325/1750 train_time:36145ms step_avg:111.22ms
step:326/1750 train_time:36250ms step_avg:111.20ms
step:327/1750 train_time:36360ms step_avg:111.19ms
step:328/1750 train_time:36465ms step_avg:111.18ms
step:329/1750 train_time:36571ms step_avg:111.16ms
step:330/1750 train_time:36676ms step_avg:111.14ms
step:331/1750 train_time:36782ms step_avg:111.12ms
step:332/1750 train_time:36891ms step_avg:111.12ms
step:333/1750 train_time:37001ms step_avg:111.11ms
step:334/1750 train_time:37116ms step_avg:111.13ms
step:335/1750 train_time:37221ms step_avg:111.11ms
step:336/1750 train_time:37326ms step_avg:111.09ms
step:337/1750 train_time:37432ms step_avg:111.07ms
step:338/1750 train_time:37537ms step_avg:111.06ms
step:339/1750 train_time:37646ms step_avg:111.05ms
step:340/1750 train_time:37752ms step_avg:111.03ms
step:341/1750 train_time:37857ms step_avg:111.02ms
step:342/1750 train_time:37970ms step_avg:111.02ms
step:343/1750 train_time:38083ms step_avg:111.03ms
step:344/1750 train_time:38205ms step_avg:111.06ms
step:345/1750 train_time:38317ms step_avg:111.06ms
step:346/1750 train_time:38426ms step_avg:111.06ms
step:347/1750 train_time:38544ms step_avg:111.08ms
step:348/1750 train_time:38664ms step_avg:111.10ms
step:349/1750 train_time:38777ms step_avg:111.11ms
step:350/1750 train_time:38886ms step_avg:111.10ms
step:351/1750 train_time:38991ms step_avg:111.09ms
step:352/1750 train_time:39096ms step_avg:111.07ms
step:353/1750 train_time:39210ms step_avg:111.08ms
step:354/1750 train_time:39321ms step_avg:111.08ms
step:355/1750 train_time:39427ms step_avg:111.06ms
step:356/1750 train_time:39532ms step_avg:111.05ms
step:357/1750 train_time:39637ms step_avg:111.03ms
step:358/1750 train_time:39766ms step_avg:111.08ms
step:359/1750 train_time:39878ms step_avg:111.08ms
step:360/1750 train_time:39983ms step_avg:111.06ms
step:361/1750 train_time:40090ms step_avg:111.05ms
step:362/1750 train_time:40194ms step_avg:111.03ms
step:363/1750 train_time:40299ms step_avg:111.02ms
step:364/1750 train_time:40405ms step_avg:111.00ms
step:365/1750 train_time:40510ms step_avg:110.99ms
step:366/1750 train_time:40615ms step_avg:110.97ms
step:367/1750 train_time:40721ms step_avg:110.96ms
step:368/1750 train_time:40826ms step_avg:110.94ms
step:369/1750 train_time:40932ms step_avg:110.93ms
step:370/1750 train_time:41037ms step_avg:110.91ms
step:371/1750 train_time:41142ms step_avg:110.89ms
step:372/1750 train_time:41247ms step_avg:110.88ms
step:373/1750 train_time:41353ms step_avg:110.87ms
step:374/1750 train_time:41458ms step_avg:110.85ms
step:375/1750 train_time:41564ms step_avg:110.84ms
step:375/1750 val_loss:3.8960 train_time:41665ms step_avg:111.11ms
step:376/1750 train_time:41728ms step_avg:110.98ms
step:377/1750 train_time:41851ms step_avg:111.01ms
step:378/1750 train_time:41993ms step_avg:111.09ms
step:379/1750 train_time:42107ms step_avg:111.10ms
step:380/1750 train_time:42223ms step_avg:111.11ms
step:381/1750 train_time:42330ms step_avg:111.10ms
step:382/1750 train_time:42445ms step_avg:111.11ms
step:383/1750 train_time:42557ms step_avg:111.12ms
step:384/1750 train_time:42670ms step_avg:111.12ms
step:385/1750 train_time:42782ms step_avg:111.12ms
step:386/1750 train_time:42895ms step_avg:111.13ms
step:387/1750 train_time:43007ms step_avg:111.13ms
step:388/1750 train_time:43127ms step_avg:111.15ms
step:389/1750 train_time:43243ms step_avg:111.16ms
step:390/1750 train_time:43355ms step_avg:111.17ms
step:391/1750 train_time:43475ms step_avg:111.19ms
step:392/1750 train_time:43594ms step_avg:111.21ms
step:393/1750 train_time:43720ms step_avg:111.25ms
step:394/1750 train_time:43821ms step_avg:111.22ms
step:395/1750 train_time:43932ms step_avg:111.22ms
step:396/1750 train_time:44046ms step_avg:111.23ms
step:397/1750 train_time:44165ms step_avg:111.25ms
step:398/1750 train_time:44270ms step_avg:111.23ms
step:399/1750 train_time:44380ms step_avg:111.23ms
step:400/1750 train_time:44493ms step_avg:111.23ms
step:401/1750 train_time:44607ms step_avg:111.24ms
step:402/1750 train_time:44721ms step_avg:111.25ms
step:403/1750 train_time:44834ms step_avg:111.25ms
step:404/1750 train_time:44948ms step_avg:111.26ms
step:405/1750 train_time:45061ms step_avg:111.26ms
step:406/1750 train_time:45175ms step_avg:111.27ms
step:407/1750 train_time:45289ms step_avg:111.27ms
step:408/1750 train_time:45403ms step_avg:111.28ms
step:409/1750 train_time:45516ms step_avg:111.29ms
step:410/1750 train_time:45631ms step_avg:111.30ms
step:411/1750 train_time:45743ms step_avg:111.30ms
step:412/1750 train_time:45857ms step_avg:111.30ms
step:413/1750 train_time:45973ms step_avg:111.32ms
step:414/1750 train_time:46085ms step_avg:111.32ms
step:415/1750 train_time:46198ms step_avg:111.32ms
step:416/1750 train_time:46312ms step_avg:111.33ms
step:417/1750 train_time:46425ms step_avg:111.33ms
step:418/1750 train_time:46539ms step_avg:111.34ms
step:419/1750 train_time:46654ms step_avg:111.35ms
step:420/1750 train_time:46770ms step_avg:111.36ms
step:421/1750 train_time:46884ms step_avg:111.36ms
step:422/1750 train_time:46996ms step_avg:111.37ms
step:423/1750 train_time:47117ms step_avg:111.39ms
step:424/1750 train_time:47232ms step_avg:111.40ms
step:425/1750 train_time:47346ms step_avg:111.40ms
step:426/1750 train_time:47460ms step_avg:111.41ms
step:427/1750 train_time:47572ms step_avg:111.41ms
step:428/1750 train_time:47685ms step_avg:111.41ms
step:429/1750 train_time:47800ms step_avg:111.42ms
step:430/1750 train_time:47914ms step_avg:111.43ms
step:431/1750 train_time:48027ms step_avg:111.43ms
step:432/1750 train_time:48141ms step_avg:111.44ms
step:433/1750 train_time:48253ms step_avg:111.44ms
step:434/1750 train_time:48366ms step_avg:111.44ms
step:435/1750 train_time:48480ms step_avg:111.45ms
step:436/1750 train_time:48594ms step_avg:111.45ms
step:437/1750 train_time:48709ms step_avg:111.46ms
step:438/1750 train_time:48822ms step_avg:111.47ms
step:439/1750 train_time:48936ms step_avg:111.47ms
step:440/1750 train_time:49050ms step_avg:111.48ms
step:441/1750 train_time:49165ms step_avg:111.49ms
step:442/1750 train_time:49277ms step_avg:111.49ms
step:443/1750 train_time:49391ms step_avg:111.49ms
step:444/1750 train_time:49504ms step_avg:111.50ms
step:445/1750 train_time:49618ms step_avg:111.50ms
step:446/1750 train_time:49732ms step_avg:111.51ms
step:447/1750 train_time:49861ms step_avg:111.55ms
step:448/1750 train_time:49981ms step_avg:111.57ms
step:449/1750 train_time:50109ms step_avg:111.60ms
step:450/1750 train_time:50222ms step_avg:111.61ms
step:451/1750 train_time:50336ms step_avg:111.61ms
step:452/1750 train_time:50449ms step_avg:111.61ms
step:453/1750 train_time:50563ms step_avg:111.62ms
step:454/1750 train_time:50676ms step_avg:111.62ms
step:455/1750 train_time:50790ms step_avg:111.63ms
step:456/1750 train_time:50910ms step_avg:111.64ms
step:457/1750 train_time:51023ms step_avg:111.65ms
step:458/1750 train_time:51137ms step_avg:111.65ms
step:459/1750 train_time:51250ms step_avg:111.66ms
step:460/1750 train_time:51364ms step_avg:111.66ms
step:461/1750 train_time:51478ms step_avg:111.66ms
step:462/1750 train_time:51591ms step_avg:111.67ms
step:463/1750 train_time:51705ms step_avg:111.67ms
step:464/1750 train_time:51818ms step_avg:111.68ms
step:465/1750 train_time:51932ms step_avg:111.68ms
step:466/1750 train_time:52045ms step_avg:111.69ms
step:467/1750 train_time:52159ms step_avg:111.69ms
step:468/1750 train_time:52269ms step_avg:111.69ms
step:469/1750 train_time:52376ms step_avg:111.68ms
step:470/1750 train_time:52482ms step_avg:111.66ms
step:471/1750 train_time:52589ms step_avg:111.65ms
step:472/1750 train_time:52696ms step_avg:111.64ms
step:473/1750 train_time:52808ms step_avg:111.64ms
step:474/1750 train_time:52928ms step_avg:111.66ms
step:475/1750 train_time:53024ms step_avg:111.63ms
step:476/1750 train_time:53122ms step_avg:111.60ms
step:477/1750 train_time:53251ms step_avg:111.64ms
step:478/1750 train_time:53336ms step_avg:111.58ms
step:479/1750 train_time:53442ms step_avg:111.57ms
step:480/1750 train_time:53549ms step_avg:111.56ms
step:481/1750 train_time:53656ms step_avg:111.55ms
step:482/1750 train_time:53762ms step_avg:111.54ms
step:483/1750 train_time:53869ms step_avg:111.53ms
step:484/1750 train_time:53979ms step_avg:111.53ms
step:485/1750 train_time:54098ms step_avg:111.54ms
step:486/1750 train_time:54200ms step_avg:111.52ms
step:487/1750 train_time:54295ms step_avg:111.49ms
step:488/1750 train_time:54402ms step_avg:111.48ms
step:489/1750 train_time:54508ms step_avg:111.47ms
step:490/1750 train_time:54615ms step_avg:111.46ms
step:491/1750 train_time:54841ms step_avg:111.69ms
step:492/1750 train_time:54955ms step_avg:111.70ms
step:493/1750 train_time:55073ms step_avg:111.71ms
step:494/1750 train_time:55189ms step_avg:111.72ms
step:495/1750 train_time:55296ms step_avg:111.71ms
step:496/1750 train_time:55402ms step_avg:111.70ms
step:497/1750 train_time:55531ms step_avg:111.73ms
step:498/1750 train_time:55622ms step_avg:111.69ms
step:499/1750 train_time:55746ms step_avg:111.72ms
step:500/1750 train_time:55845ms step_avg:111.69ms
step:500/1750 val_loss:3.7473 train_time:55948ms step_avg:111.90ms
step:501/1750 train_time:56001ms step_avg:111.78ms
step:502/1750 train_time:56128ms step_avg:111.81ms
step:503/1750 train_time:56241ms step_avg:111.81ms
step:504/1750 train_time:56348ms step_avg:111.80ms
step:505/1750 train_time:56455ms step_avg:111.79ms
step:506/1750 train_time:56561ms step_avg:111.78ms
step:507/1750 train_time:56668ms step_avg:111.77ms
step:508/1750 train_time:56775ms step_avg:111.76ms
step:509/1750 train_time:56881ms step_avg:111.75ms
step:510/1750 train_time:56988ms step_avg:111.74ms
step:511/1750 train_time:57095ms step_avg:111.73ms
step:512/1750 train_time:57201ms step_avg:111.72ms
step:513/1750 train_time:57315ms step_avg:111.72ms
step:514/1750 train_time:57429ms step_avg:111.73ms
step:515/1750 train_time:57542ms step_avg:111.73ms
step:516/1750 train_time:57656ms step_avg:111.74ms
step:517/1750 train_time:57769ms step_avg:111.74ms
step:518/1750 train_time:57883ms step_avg:111.74ms
step:519/1750 train_time:57996ms step_avg:111.75ms
step:520/1750 train_time:58110ms step_avg:111.75ms
step:521/1750 train_time:58224ms step_avg:111.76ms
step:522/1750 train_time:58338ms step_avg:111.76ms
step:523/1750 train_time:58452ms step_avg:111.76ms
step:524/1750 train_time:58566ms step_avg:111.77ms
step:525/1750 train_time:58680ms step_avg:111.77ms
step:526/1750 train_time:58793ms step_avg:111.77ms
step:527/1750 train_time:58908ms step_avg:111.78ms
step:528/1750 train_time:59024ms step_avg:111.79ms
step:529/1750 train_time:59138ms step_avg:111.79ms
step:530/1750 train_time:59252ms step_avg:111.80ms
step:531/1750 train_time:59381ms step_avg:111.83ms
step:532/1750 train_time:59479ms step_avg:111.80ms
step:533/1750 train_time:59591ms step_avg:111.80ms
step:534/1750 train_time:59705ms step_avg:111.81ms
step:535/1750 train_time:59820ms step_avg:111.81ms
step:536/1750 train_time:59933ms step_avg:111.82ms
step:537/1750 train_time:60047ms step_avg:111.82ms
step:538/1750 train_time:60163ms step_avg:111.83ms
step:539/1750 train_time:60275ms step_avg:111.83ms
step:540/1750 train_time:60389ms step_avg:111.83ms
step:541/1750 train_time:60503ms step_avg:111.84ms
step:542/1750 train_time:60617ms step_avg:111.84ms
step:543/1750 train_time:60732ms step_avg:111.85ms
step:544/1750 train_time:60846ms step_avg:111.85ms
step:545/1750 train_time:60958ms step_avg:111.85ms
step:546/1750 train_time:61072ms step_avg:111.85ms
step:547/1750 train_time:61186ms step_avg:111.86ms
step:548/1750 train_time:61300ms step_avg:111.86ms
step:549/1750 train_time:61413ms step_avg:111.86ms
step:550/1750 train_time:61527ms step_avg:111.87ms
step:551/1750 train_time:61641ms step_avg:111.87ms
step:552/1750 train_time:61756ms step_avg:111.88ms
step:553/1750 train_time:61870ms step_avg:111.88ms
step:554/1750 train_time:61992ms step_avg:111.90ms
step:555/1750 train_time:62106ms step_avg:111.90ms
step:556/1750 train_time:62221ms step_avg:111.91ms
step:557/1750 train_time:62334ms step_avg:111.91ms
step:558/1750 train_time:62447ms step_avg:111.91ms
step:559/1750 train_time:62561ms step_avg:111.92ms
step:560/1750 train_time:62675ms step_avg:111.92ms
step:561/1750 train_time:62789ms step_avg:111.92ms
step:562/1750 train_time:62905ms step_avg:111.93ms
step:563/1750 train_time:63022ms step_avg:111.94ms
step:564/1750 train_time:63130ms step_avg:111.93ms
step:565/1750 train_time:63244ms step_avg:111.94ms
step:566/1750 train_time:63358ms step_avg:111.94ms
step:567/1750 train_time:63472ms step_avg:111.94ms
step:568/1750 train_time:63586ms step_avg:111.95ms
step:569/1750 train_time:63699ms step_avg:111.95ms
step:570/1750 train_time:63813ms step_avg:111.95ms
step:571/1750 train_time:63928ms step_avg:111.96ms
step:572/1750 train_time:64071ms step_avg:112.01ms
step:573/1750 train_time:64189ms step_avg:112.02ms
step:574/1750 train_time:64296ms step_avg:112.01ms
step:575/1750 train_time:64404ms step_avg:112.01ms
step:576/1750 train_time:64511ms step_avg:112.00ms
step:577/1750 train_time:64618ms step_avg:111.99ms
step:578/1750 train_time:64725ms step_avg:111.98ms
step:579/1750 train_time:64834ms step_avg:111.98ms
step:580/1750 train_time:64941ms step_avg:111.97ms
step:581/1750 train_time:65048ms step_avg:111.96ms
step:582/1750 train_time:65155ms step_avg:111.95ms
step:583/1750 train_time:65262ms step_avg:111.94ms
step:584/1750 train_time:65368ms step_avg:111.93ms
step:585/1750 train_time:65475ms step_avg:111.92ms
step:586/1750 train_time:65582ms step_avg:111.92ms
step:587/1750 train_time:65689ms step_avg:111.91ms
step:588/1750 train_time:65796ms step_avg:111.90ms
step:589/1750 train_time:65903ms step_avg:111.89ms
step:590/1750 train_time:66010ms step_avg:111.88ms
step:591/1750 train_time:66117ms step_avg:111.87ms
step:592/1750 train_time:66224ms step_avg:111.86ms
step:593/1750 train_time:66331ms step_avg:111.86ms
step:594/1750 train_time:66437ms step_avg:111.85ms
step:595/1750 train_time:66545ms step_avg:111.84ms
step:596/1750 train_time:66652ms step_avg:111.83ms
step:597/1750 train_time:66763ms step_avg:111.83ms
step:598/1750 train_time:66865ms step_avg:111.81ms
step:599/1750 train_time:66977ms step_avg:111.81ms
step:600/1750 train_time:67093ms step_avg:111.82ms
step:601/1750 train_time:67186ms step_avg:111.79ms
step:602/1750 train_time:67293ms step_avg:111.78ms
step:603/1750 train_time:67400ms step_avg:111.77ms
step:604/1750 train_time:67507ms step_avg:111.77ms
step:605/1750 train_time:67614ms step_avg:111.76ms
step:606/1750 train_time:67721ms step_avg:111.75ms
step:607/1750 train_time:67828ms step_avg:111.74ms
step:608/1750 train_time:67934ms step_avg:111.73ms
step:609/1750 train_time:68041ms step_avg:111.73ms
step:610/1750 train_time:68148ms step_avg:111.72ms
step:611/1750 train_time:68255ms step_avg:111.71ms
step:612/1750 train_time:68362ms step_avg:111.70ms
step:613/1750 train_time:68469ms step_avg:111.69ms
step:614/1750 train_time:68575ms step_avg:111.69ms
step:615/1750 train_time:68682ms step_avg:111.68ms
step:616/1750 train_time:68789ms step_avg:111.67ms
step:617/1750 train_time:68896ms step_avg:111.66ms
step:618/1750 train_time:69003ms step_avg:111.66ms
step:619/1750 train_time:69111ms step_avg:111.65ms
step:620/1750 train_time:69217ms step_avg:111.64ms
step:621/1750 train_time:69324ms step_avg:111.63ms
step:622/1750 train_time:69431ms step_avg:111.62ms
step:623/1750 train_time:69538ms step_avg:111.62ms
step:624/1750 train_time:69644ms step_avg:111.61ms
step:625/1750 train_time:69751ms step_avg:111.60ms
step:625/1750 val_loss:3.6609 train_time:69854ms step_avg:111.77ms
step:626/1750 train_time:69894ms step_avg:111.65ms
step:627/1750 train_time:70023ms step_avg:111.68ms
step:628/1750 train_time:70161ms step_avg:111.72ms
step:629/1750 train_time:70300ms step_avg:111.76ms
step:630/1750 train_time:70429ms step_avg:111.79ms
step:631/1750 train_time:70544ms step_avg:111.80ms
step:632/1750 train_time:70659ms step_avg:111.80ms
step:633/1750 train_time:70805ms step_avg:111.86ms
step:634/1750 train_time:70932ms step_avg:111.88ms
step:635/1750 train_time:71068ms step_avg:111.92ms
step:636/1750 train_time:71203ms step_avg:111.95ms
step:637/1750 train_time:71337ms step_avg:111.99ms
step:638/1750 train_time:71459ms step_avg:112.00ms
step:639/1750 train_time:71584ms step_avg:112.02ms
step:640/1750 train_time:71709ms step_avg:112.05ms
step:641/1750 train_time:71837ms step_avg:112.07ms
step:642/1750 train_time:71953ms step_avg:112.08ms
step:643/1750 train_time:72068ms step_avg:112.08ms
step:644/1750 train_time:72181ms step_avg:112.08ms
step:645/1750 train_time:72295ms step_avg:112.08ms
step:646/1750 train_time:72409ms step_avg:112.09ms
step:647/1750 train_time:72523ms step_avg:112.09ms
step:648/1750 train_time:72637ms step_avg:112.09ms
step:649/1750 train_time:72751ms step_avg:112.10ms
step:650/1750 train_time:72865ms step_avg:112.10ms
step:651/1750 train_time:72982ms step_avg:112.11ms
step:652/1750 train_time:73097ms step_avg:112.11ms
step:653/1750 train_time:73212ms step_avg:112.12ms
step:654/1750 train_time:73327ms step_avg:112.12ms
step:655/1750 train_time:73442ms step_avg:112.13ms
step:656/1750 train_time:73557ms step_avg:112.13ms
step:657/1750 train_time:73672ms step_avg:112.13ms
step:658/1750 train_time:73787ms step_avg:112.14ms
step:659/1750 train_time:73904ms step_avg:112.15ms
step:660/1750 train_time:74018ms step_avg:112.15ms
step:661/1750 train_time:74126ms step_avg:112.14ms
step:662/1750 train_time:74234ms step_avg:112.14ms
step:663/1750 train_time:74342ms step_avg:112.13ms
step:664/1750 train_time:74450ms step_avg:112.12ms
step:665/1750 train_time:74559ms step_avg:112.12ms
step:666/1750 train_time:74667ms step_avg:112.11ms
step:667/1750 train_time:74775ms step_avg:112.11ms
step:668/1750 train_time:74883ms step_avg:112.10ms
step:669/1750 train_time:74991ms step_avg:112.09ms
step:670/1750 train_time:75099ms step_avg:112.09ms
step:671/1750 train_time:75208ms step_avg:112.08ms
step:672/1750 train_time:75319ms step_avg:112.08ms
step:673/1750 train_time:75424ms step_avg:112.07ms
step:674/1750 train_time:75532ms step_avg:112.07ms
step:675/1750 train_time:75640ms step_avg:112.06ms
step:676/1750 train_time:75749ms step_avg:112.05ms
step:677/1750 train_time:75857ms step_avg:112.05ms
step:678/1750 train_time:75965ms step_avg:112.04ms
step:679/1750 train_time:76077ms step_avg:112.04ms
step:680/1750 train_time:76182ms step_avg:112.03ms
step:681/1750 train_time:76289ms step_avg:112.03ms
step:682/1750 train_time:76397ms step_avg:112.02ms
step:683/1750 train_time:76506ms step_avg:112.01ms
step:684/1750 train_time:76614ms step_avg:112.01ms
step:685/1750 train_time:76722ms step_avg:112.00ms
step:686/1750 train_time:76831ms step_avg:112.00ms
step:687/1750 train_time:76938ms step_avg:111.99ms
step:688/1750 train_time:77068ms step_avg:112.02ms
step:689/1750 train_time:77158ms step_avg:111.99ms
step:690/1750 train_time:77262ms step_avg:111.97ms
step:691/1750 train_time:77371ms step_avg:111.97ms
step:692/1750 train_time:77479ms step_avg:111.96ms
step:693/1750 train_time:77587ms step_avg:111.96ms
step:694/1750 train_time:77695ms step_avg:111.95ms
step:695/1750 train_time:77803ms step_avg:111.95ms
step:696/1750 train_time:77911ms step_avg:111.94ms
step:697/1750 train_time:78020ms step_avg:111.94ms
step:698/1750 train_time:78128ms step_avg:111.93ms
step:699/1750 train_time:78236ms step_avg:111.93ms
step:700/1750 train_time:78344ms step_avg:111.92ms
step:701/1750 train_time:78452ms step_avg:111.91ms
step:702/1750 train_time:78566ms step_avg:111.92ms
step:703/1750 train_time:78674ms step_avg:111.91ms
step:704/1750 train_time:78782ms step_avg:111.91ms
step:705/1750 train_time:78890ms step_avg:111.90ms
step:706/1750 train_time:78999ms step_avg:111.90ms
step:707/1750 train_time:79107ms step_avg:111.89ms
step:708/1750 train_time:79215ms step_avg:111.89ms
step:709/1750 train_time:79325ms step_avg:111.88ms
step:710/1750 train_time:79433ms step_avg:111.88ms
step:711/1750 train_time:79541ms step_avg:111.87ms
step:712/1750 train_time:79649ms step_avg:111.87ms
step:713/1750 train_time:79758ms step_avg:111.86ms
step:714/1750 train_time:79866ms step_avg:111.86ms
step:715/1750 train_time:79974ms step_avg:111.85ms
step:716/1750 train_time:80082ms step_avg:111.85ms
step:717/1750 train_time:80190ms step_avg:111.84ms
step:718/1750 train_time:80298ms step_avg:111.84ms
step:719/1750 train_time:80406ms step_avg:111.83ms
step:720/1750 train_time:80514ms step_avg:111.83ms
step:721/1750 train_time:80622ms step_avg:111.82ms
step:722/1750 train_time:80734ms step_avg:111.82ms
step:723/1750 train_time:80839ms step_avg:111.81ms
step:724/1750 train_time:80950ms step_avg:111.81ms
step:725/1750 train_time:81055ms step_avg:111.80ms
step:726/1750 train_time:81163ms step_avg:111.79ms
step:727/1750 train_time:81271ms step_avg:111.79ms
step:728/1750 train_time:81379ms step_avg:111.78ms
step:729/1750 train_time:81487ms step_avg:111.78ms
step:730/1750 train_time:81595ms step_avg:111.77ms
step:731/1750 train_time:81703ms step_avg:111.77ms
step:732/1750 train_time:81811ms step_avg:111.76ms
step:733/1750 train_time:81919ms step_avg:111.76ms
step:734/1750 train_time:82042ms step_avg:111.77ms
step:735/1750 train_time:82135ms step_avg:111.75ms
step:736/1750 train_time:82367ms step_avg:111.91ms
step:737/1750 train_time:82495ms step_avg:111.93ms
step:738/1750 train_time:82620ms step_avg:111.95ms
step:739/1750 train_time:82728ms step_avg:111.95ms
step:740/1750 train_time:82836ms step_avg:111.94ms
step:741/1750 train_time:82944ms step_avg:111.94ms
step:742/1750 train_time:83058ms step_avg:111.94ms
step:743/1750 train_time:83171ms step_avg:111.94ms
step:744/1750 train_time:83285ms step_avg:111.94ms
step:745/1750 train_time:83383ms step_avg:111.92ms
step:746/1750 train_time:83492ms step_avg:111.92ms
step:747/1750 train_time:83600ms step_avg:111.91ms
step:748/1750 train_time:83708ms step_avg:111.91ms
step:749/1750 train_time:83816ms step_avg:111.90ms
step:750/1750 train_time:83924ms step_avg:111.90ms
step:750/1750 val_loss:3.5950 train_time:84029ms step_avg:112.04ms
step:751/1750 train_time:84048ms step_avg:111.92ms
step:752/1750 train_time:84204ms step_avg:111.97ms
step:753/1750 train_time:84339ms step_avg:112.00ms
step:754/1750 train_time:84489ms step_avg:112.05ms
step:755/1750 train_time:84623ms step_avg:112.08ms
step:756/1750 train_time:84739ms step_avg:112.09ms
step:757/1750 train_time:84855ms step_avg:112.09ms
step:758/1750 train_time:84971ms step_avg:112.10ms
step:759/1750 train_time:85098ms step_avg:112.12ms
step:760/1750 train_time:85215ms step_avg:112.13ms
step:761/1750 train_time:85331ms step_avg:112.13ms
step:762/1750 train_time:85446ms step_avg:112.13ms
step:763/1750 train_time:85559ms step_avg:112.13ms
step:764/1750 train_time:85675ms step_avg:112.14ms
step:765/1750 train_time:85791ms step_avg:112.14ms
step:766/1750 train_time:85907ms step_avg:112.15ms
step:767/1750 train_time:86022ms step_avg:112.15ms
step:768/1750 train_time:86159ms step_avg:112.19ms
step:769/1750 train_time:86274ms step_avg:112.19ms
step:770/1750 train_time:86390ms step_avg:112.19ms
step:771/1750 train_time:86507ms step_avg:112.20ms
step:772/1750 train_time:86624ms step_avg:112.21ms
step:773/1750 train_time:86739ms step_avg:112.21ms
step:774/1750 train_time:86860ms step_avg:112.22ms
step:775/1750 train_time:86975ms step_avg:112.23ms
step:776/1750 train_time:87091ms step_avg:112.23ms
step:777/1750 train_time:87217ms step_avg:112.25ms
step:778/1750 train_time:87344ms step_avg:112.27ms
step:779/1750 train_time:87470ms step_avg:112.29ms
step:780/1750 train_time:87586ms step_avg:112.29ms
step:781/1750 train_time:87701ms step_avg:112.29ms
step:782/1750 train_time:87817ms step_avg:112.30ms
step:783/1750 train_time:87932ms step_avg:112.30ms
step:784/1750 train_time:88048ms step_avg:112.31ms
step:785/1750 train_time:88163ms step_avg:112.31ms
step:786/1750 train_time:88280ms step_avg:112.32ms
step:787/1750 train_time:88397ms step_avg:112.32ms
step:788/1750 train_time:88522ms step_avg:112.34ms
step:789/1750 train_time:88637ms step_avg:112.34ms
step:790/1750 train_time:88781ms step_avg:112.38ms
step:791/1750 train_time:88895ms step_avg:112.38ms
step:792/1750 train_time:89004ms step_avg:112.38ms
step:793/1750 train_time:89112ms step_avg:112.37ms
step:794/1750 train_time:89221ms step_avg:112.37ms
step:795/1750 train_time:89332ms step_avg:112.37ms
step:796/1750 train_time:89457ms step_avg:112.38ms
step:797/1750 train_time:89583ms step_avg:112.40ms
step:798/1750 train_time:89692ms step_avg:112.40ms
step:799/1750 train_time:89808ms step_avg:112.40ms
step:800/1750 train_time:89924ms step_avg:112.40ms
step:801/1750 train_time:90047ms step_avg:112.42ms
step:802/1750 train_time:90163ms step_avg:112.42ms
step:803/1750 train_time:90298ms step_avg:112.45ms
step:804/1750 train_time:90417ms step_avg:112.46ms
step:805/1750 train_time:90554ms step_avg:112.49ms
step:806/1750 train_time:90733ms step_avg:112.57ms
step:807/1750 train_time:90878ms step_avg:112.61ms
step:808/1750 train_time:90994ms step_avg:112.62ms
step:809/1750 train_time:91110ms step_avg:112.62ms
step:810/1750 train_time:91225ms step_avg:112.62ms
step:811/1750 train_time:91345ms step_avg:112.63ms
step:812/1750 train_time:91460ms step_avg:112.64ms
step:813/1750 train_time:91575ms step_avg:112.64ms
step:814/1750 train_time:91691ms step_avg:112.64ms
step:815/1750 train_time:91816ms step_avg:112.66ms
step:816/1750 train_time:91931ms step_avg:112.66ms
step:817/1750 train_time:92047ms step_avg:112.66ms
step:818/1750 train_time:92162ms step_avg:112.67ms
step:819/1750 train_time:92285ms step_avg:112.68ms
step:820/1750 train_time:92398ms step_avg:112.68ms
step:821/1750 train_time:92527ms step_avg:112.70ms
step:822/1750 train_time:92656ms step_avg:112.72ms
step:823/1750 train_time:92788ms step_avg:112.74ms
step:824/1750 train_time:92928ms step_avg:112.78ms
step:825/1750 train_time:93045ms step_avg:112.78ms
step:826/1750 train_time:93153ms step_avg:112.78ms
step:827/1750 train_time:93272ms step_avg:112.78ms
step:828/1750 train_time:93388ms step_avg:112.79ms
step:829/1750 train_time:93510ms step_avg:112.80ms
step:830/1750 train_time:93625ms step_avg:112.80ms
step:831/1750 train_time:93741ms step_avg:112.80ms
step:832/1750 train_time:93868ms step_avg:112.82ms
step:833/1750 train_time:93995ms step_avg:112.84ms
step:834/1750 train_time:94121ms step_avg:112.85ms
step:835/1750 train_time:94247ms step_avg:112.87ms
step:836/1750 train_time:94371ms step_avg:112.88ms
step:837/1750 train_time:94497ms step_avg:112.90ms
step:838/1750 train_time:94623ms step_avg:112.92ms
step:839/1750 train_time:94744ms step_avg:112.92ms
step:840/1750 train_time:94858ms step_avg:112.93ms
step:841/1750 train_time:94975ms step_avg:112.93ms
step:842/1750 train_time:95106ms step_avg:112.95ms
step:843/1750 train_time:95218ms step_avg:112.95ms
step:844/1750 train_time:95330ms step_avg:112.95ms
step:845/1750 train_time:95447ms step_avg:112.95ms
step:846/1750 train_time:95563ms step_avg:112.96ms
step:847/1750 train_time:95678ms step_avg:112.96ms
step:848/1750 train_time:95793ms step_avg:112.96ms
step:849/1750 train_time:95905ms step_avg:112.96ms
step:850/1750 train_time:96020ms step_avg:112.97ms
step:851/1750 train_time:96136ms step_avg:112.97ms
step:852/1750 train_time:96251ms step_avg:112.97ms
step:853/1750 train_time:96366ms step_avg:112.97ms
step:854/1750 train_time:96481ms step_avg:112.98ms
step:855/1750 train_time:96597ms step_avg:112.98ms
step:856/1750 train_time:96712ms step_avg:112.98ms
step:857/1750 train_time:96824ms step_avg:112.98ms
step:858/1750 train_time:96939ms step_avg:112.98ms
step:859/1750 train_time:97055ms step_avg:112.99ms
step:860/1750 train_time:97170ms step_avg:112.99ms
step:861/1750 train_time:97285ms step_avg:112.99ms
step:862/1750 train_time:97413ms step_avg:113.01ms
step:863/1750 train_time:97516ms step_avg:113.00ms
step:864/1750 train_time:97631ms step_avg:113.00ms
step:865/1750 train_time:97746ms step_avg:113.00ms
step:866/1750 train_time:97862ms step_avg:113.00ms
step:867/1750 train_time:97977ms step_avg:113.01ms
step:868/1750 train_time:98090ms step_avg:113.01ms
step:869/1750 train_time:98202ms step_avg:113.01ms
step:870/1750 train_time:98317ms step_avg:113.01ms
step:871/1750 train_time:98433ms step_avg:113.01ms
step:872/1750 train_time:98548ms step_avg:113.01ms
step:873/1750 train_time:98664ms step_avg:113.02ms
step:874/1750 train_time:98779ms step_avg:113.02ms
step:875/1750 train_time:98887ms step_avg:113.01ms
step:875/1750 val_loss:3.5476 train_time:98992ms step_avg:113.13ms
step:876/1750 train_time:99015ms step_avg:113.03ms
step:877/1750 train_time:99182ms step_avg:113.09ms
step:878/1750 train_time:99318ms step_avg:113.12ms
step:879/1750 train_time:99437ms step_avg:113.12ms
step:880/1750 train_time:99552ms step_avg:113.13ms
step:881/1750 train_time:99668ms step_avg:113.13ms
step:882/1750 train_time:99784ms step_avg:113.13ms
step:883/1750 train_time:99901ms step_avg:113.14ms
step:884/1750 train_time:100014ms step_avg:113.14ms
step:885/1750 train_time:100130ms step_avg:113.14ms
step:886/1750 train_time:100245ms step_avg:113.14ms
step:887/1750 train_time:100361ms step_avg:113.15ms
step:888/1750 train_time:100478ms step_avg:113.15ms
step:889/1750 train_time:100604ms step_avg:113.17ms
step:890/1750 train_time:100720ms step_avg:113.17ms
step:891/1750 train_time:100836ms step_avg:113.17ms
step:892/1750 train_time:100952ms step_avg:113.17ms
step:893/1750 train_time:101067ms step_avg:113.18ms
step:894/1750 train_time:101182ms step_avg:113.18ms
step:895/1750 train_time:101298ms step_avg:113.18ms
step:896/1750 train_time:101425ms step_avg:113.20ms
step:897/1750 train_time:101556ms step_avg:113.22ms
step:898/1750 train_time:101677ms step_avg:113.23ms
step:899/1750 train_time:101793ms step_avg:113.23ms
step:900/1750 train_time:101904ms step_avg:113.23ms
step:901/1750 train_time:102021ms step_avg:113.23ms
step:902/1750 train_time:102137ms step_avg:113.23ms
step:903/1750 train_time:102252ms step_avg:113.24ms
step:904/1750 train_time:102364ms step_avg:113.23ms
step:905/1750 train_time:102497ms step_avg:113.26ms
step:906/1750 train_time:102626ms step_avg:113.27ms
step:907/1750 train_time:102746ms step_avg:113.28ms
step:908/1750 train_time:102861ms step_avg:113.28ms
step:909/1750 train_time:102973ms step_avg:113.28ms
step:910/1750 train_time:103089ms step_avg:113.29ms
step:911/1750 train_time:103206ms step_avg:113.29ms
step:912/1750 train_time:103322ms step_avg:113.29ms
step:913/1750 train_time:103444ms step_avg:113.30ms
step:914/1750 train_time:103558ms step_avg:113.30ms
step:915/1750 train_time:103682ms step_avg:113.31ms
step:916/1750 train_time:103799ms step_avg:113.32ms
step:917/1750 train_time:103915ms step_avg:113.32ms
step:918/1750 train_time:104031ms step_avg:113.32ms
step:919/1750 train_time:104148ms step_avg:113.33ms
step:920/1750 train_time:104265ms step_avg:113.33ms
step:921/1750 train_time:104381ms step_avg:113.33ms
step:922/1750 train_time:104498ms step_avg:113.34ms
step:923/1750 train_time:104614ms step_avg:113.34ms
step:924/1750 train_time:104731ms step_avg:113.35ms
step:925/1750 train_time:104847ms step_avg:113.35ms
step:926/1750 train_time:104965ms step_avg:113.35ms
step:927/1750 train_time:105080ms step_avg:113.36ms
step:928/1750 train_time:105197ms step_avg:113.36ms
step:929/1750 train_time:105311ms step_avg:113.36ms
step:930/1750 train_time:105424ms step_avg:113.36ms
step:931/1750 train_time:105536ms step_avg:113.36ms
step:932/1750 train_time:105649ms step_avg:113.36ms
step:933/1750 train_time:105766ms step_avg:113.36ms
step:934/1750 train_time:105883ms step_avg:113.36ms
step:935/1750 train_time:106001ms step_avg:113.37ms
step:936/1750 train_time:106116ms step_avg:113.37ms
step:937/1750 train_time:106234ms step_avg:113.38ms
step:938/1750 train_time:106350ms step_avg:113.38ms
step:939/1750 train_time:106466ms step_avg:113.38ms
step:940/1750 train_time:106583ms step_avg:113.39ms
step:941/1750 train_time:106700ms step_avg:113.39ms
step:942/1750 train_time:106816ms step_avg:113.39ms
step:943/1750 train_time:106933ms step_avg:113.40ms
step:944/1750 train_time:107048ms step_avg:113.40ms
step:945/1750 train_time:107158ms step_avg:113.39ms
step:946/1750 train_time:107270ms step_avg:113.39ms
step:947/1750 train_time:107382ms step_avg:113.39ms
step:948/1750 train_time:107491ms step_avg:113.39ms
step:949/1750 train_time:107601ms step_avg:113.38ms
step:950/1750 train_time:107711ms step_avg:113.38ms
step:951/1750 train_time:107821ms step_avg:113.38ms
step:952/1750 train_time:107930ms step_avg:113.37ms
step:953/1750 train_time:108040ms step_avg:113.37ms
step:954/1750 train_time:108149ms step_avg:113.36ms
step:955/1750 train_time:108258ms step_avg:113.36ms
step:956/1750 train_time:108368ms step_avg:113.36ms
step:957/1750 train_time:108478ms step_avg:113.35ms
step:958/1750 train_time:108587ms step_avg:113.35ms
step:959/1750 train_time:108697ms step_avg:113.34ms
step:960/1750 train_time:108807ms step_avg:113.34ms
step:961/1750 train_time:108916ms step_avg:113.34ms
step:962/1750 train_time:109026ms step_avg:113.33ms
step:963/1750 train_time:109135ms step_avg:113.33ms
step:964/1750 train_time:109245ms step_avg:113.32ms
step:965/1750 train_time:109354ms step_avg:113.32ms
step:966/1750 train_time:109464ms step_avg:113.32ms
step:967/1750 train_time:109573ms step_avg:113.31ms
step:968/1750 train_time:109684ms step_avg:113.31ms
step:969/1750 train_time:109794ms step_avg:113.31ms
step:970/1750 train_time:109903ms step_avg:113.30ms
step:971/1750 train_time:110013ms step_avg:113.30ms
step:972/1750 train_time:110122ms step_avg:113.29ms
step:973/1750 train_time:110232ms step_avg:113.29ms
step:974/1750 train_time:110341ms step_avg:113.29ms
step:975/1750 train_time:110450ms step_avg:113.28ms
step:976/1750 train_time:110560ms step_avg:113.28ms
step:977/1750 train_time:110670ms step_avg:113.28ms
step:978/1750 train_time:110779ms step_avg:113.27ms
step:979/1750 train_time:110888ms step_avg:113.27ms
step:980/1750 train_time:111086ms step_avg:113.35ms
step:981/1750 train_time:111237ms step_avg:113.39ms
step:982/1750 train_time:111331ms step_avg:113.37ms
step:983/1750 train_time:111440ms step_avg:113.37ms
step:984/1750 train_time:111565ms step_avg:113.38ms
step:985/1750 train_time:111692ms step_avg:113.39ms
step:986/1750 train_time:111809ms step_avg:113.40ms
step:987/1750 train_time:111918ms step_avg:113.39ms
step:988/1750 train_time:112028ms step_avg:113.39ms
step:989/1750 train_time:112138ms step_avg:113.38ms
step:990/1750 train_time:112247ms step_avg:113.38ms
step:991/1750 train_time:112358ms step_avg:113.38ms
step:992/1750 train_time:112467ms step_avg:113.37ms
step:993/1750 train_time:112577ms step_avg:113.37ms
step:994/1750 train_time:112687ms step_avg:113.37ms
step:995/1750 train_time:112796ms step_avg:113.36ms
step:996/1750 train_time:112907ms step_avg:113.36ms
step:997/1750 train_time:113019ms step_avg:113.36ms
step:998/1750 train_time:113129ms step_avg:113.36ms
step:999/1750 train_time:113238ms step_avg:113.35ms
step:1000/1750 train_time:113348ms step_avg:113.35ms
step:1000/1750 val_loss:3.5071 train_time:113454ms step_avg:113.45ms
step:1001/1750 train_time:113501ms step_avg:113.39ms
step:1002/1750 train_time:113658ms step_avg:113.43ms
step:1003/1750 train_time:113816ms step_avg:113.48ms
step:1004/1750 train_time:113966ms step_avg:113.51ms
step:1005/1750 train_time:114083ms step_avg:113.52ms
step:1006/1750 train_time:114220ms step_avg:113.54ms
step:1007/1750 train_time:114370ms step_avg:113.57ms
step:1008/1750 train_time:114487ms step_avg:113.58ms
step:1009/1750 train_time:114603ms step_avg:113.58ms
step:1010/1750 train_time:114720ms step_avg:113.58ms
step:1011/1750 train_time:114839ms step_avg:113.59ms
step:1012/1750 train_time:114955ms step_avg:113.59ms
step:1013/1750 train_time:115073ms step_avg:113.60ms
step:1014/1750 train_time:115188ms step_avg:113.60ms
step:1015/1750 train_time:115305ms step_avg:113.60ms
step:1016/1750 train_time:115425ms step_avg:113.61ms
step:1017/1750 train_time:115541ms step_avg:113.61ms
step:1018/1750 train_time:115658ms step_avg:113.61ms
step:1019/1750 train_time:115774ms step_avg:113.62ms
step:1020/1750 train_time:115891ms step_avg:113.62ms
step:1021/1750 train_time:116029ms step_avg:113.64ms
step:1022/1750 train_time:116166ms step_avg:113.67ms
step:1023/1750 train_time:116275ms step_avg:113.66ms
step:1024/1750 train_time:116387ms step_avg:113.66ms
step:1025/1750 train_time:116496ms step_avg:113.65ms
step:1026/1750 train_time:116606ms step_avg:113.65ms
step:1027/1750 train_time:116716ms step_avg:113.65ms
step:1028/1750 train_time:116825ms step_avg:113.64ms
step:1029/1750 train_time:116936ms step_avg:113.64ms
step:1030/1750 train_time:117046ms step_avg:113.64ms
step:1031/1750 train_time:117155ms step_avg:113.63ms
step:1032/1750 train_time:117264ms step_avg:113.63ms
step:1033/1750 train_time:117382ms step_avg:113.63ms
step:1034/1750 train_time:117534ms step_avg:113.67ms
step:1035/1750 train_time:117688ms step_avg:113.71ms
step:1036/1750 train_time:117849ms step_avg:113.75ms
step:1037/1750 train_time:118013ms step_avg:113.80ms
step:1038/1750 train_time:118117ms step_avg:113.79ms
step:1039/1750 train_time:118282ms step_avg:113.84ms
step:1040/1750 train_time:118415ms step_avg:113.86ms
step:1041/1750 train_time:118533ms step_avg:113.86ms
step:1042/1750 train_time:118649ms step_avg:113.87ms
step:1043/1750 train_time:118766ms step_avg:113.87ms
step:1044/1750 train_time:118882ms step_avg:113.87ms
step:1045/1750 train_time:118999ms step_avg:113.87ms
step:1046/1750 train_time:119119ms step_avg:113.88ms
step:1047/1750 train_time:119233ms step_avg:113.88ms
step:1048/1750 train_time:119350ms step_avg:113.88ms
step:1049/1750 train_time:119463ms step_avg:113.88ms
step:1050/1750 train_time:119580ms step_avg:113.89ms
step:1051/1750 train_time:119690ms step_avg:113.88ms
step:1052/1750 train_time:119800ms step_avg:113.88ms
step:1053/1750 train_time:119912ms step_avg:113.88ms
step:1054/1750 train_time:120018ms step_avg:113.87ms
step:1055/1750 train_time:120145ms step_avg:113.88ms
step:1056/1750 train_time:120257ms step_avg:113.88ms
step:1057/1750 train_time:120363ms step_avg:113.87ms
step:1058/1750 train_time:120461ms step_avg:113.86ms
step:1059/1750 train_time:120574ms step_avg:113.86ms
step:1060/1750 train_time:120684ms step_avg:113.85ms
step:1061/1750 train_time:120818ms step_avg:113.87ms
step:1062/1750 train_time:120903ms step_avg:113.85ms
step:1063/1750 train_time:121013ms step_avg:113.84ms
step:1064/1750 train_time:121123ms step_avg:113.84ms
step:1065/1750 train_time:121234ms step_avg:113.83ms
step:1066/1750 train_time:121344ms step_avg:113.83ms
step:1067/1750 train_time:121464ms step_avg:113.84ms
step:1068/1750 train_time:121570ms step_avg:113.83ms
step:1069/1750 train_time:121687ms step_avg:113.83ms
step:1070/1750 train_time:121801ms step_avg:113.83ms
step:1071/1750 train_time:121915ms step_avg:113.83ms
step:1072/1750 train_time:122029ms step_avg:113.83ms
step:1073/1750 train_time:122149ms step_avg:113.84ms
step:1074/1750 train_time:122248ms step_avg:113.82ms
step:1075/1750 train_time:122358ms step_avg:113.82ms
step:1076/1750 train_time:122467ms step_avg:113.82ms
step:1077/1750 train_time:122577ms step_avg:113.81ms
step:1078/1750 train_time:122687ms step_avg:113.81ms
step:1079/1750 train_time:122797ms step_avg:113.81ms
step:1080/1750 train_time:122906ms step_avg:113.80ms
step:1081/1750 train_time:123017ms step_avg:113.80ms
step:1082/1750 train_time:123126ms step_avg:113.80ms
step:1083/1750 train_time:123236ms step_avg:113.79ms
step:1084/1750 train_time:123346ms step_avg:113.79ms
step:1085/1750 train_time:123457ms step_avg:113.79ms
step:1086/1750 train_time:123567ms step_avg:113.78ms
step:1087/1750 train_time:123679ms step_avg:113.78ms
step:1088/1750 train_time:123789ms step_avg:113.78ms
step:1089/1750 train_time:123899ms step_avg:113.77ms
step:1090/1750 train_time:124009ms step_avg:113.77ms
step:1091/1750 train_time:124119ms step_avg:113.77ms
step:1092/1750 train_time:124238ms step_avg:113.77ms
step:1093/1750 train_time:124338ms step_avg:113.76ms
step:1094/1750 train_time:124449ms step_avg:113.76ms
step:1095/1750 train_time:124558ms step_avg:113.75ms
step:1096/1750 train_time:124669ms step_avg:113.75ms
step:1097/1750 train_time:124779ms step_avg:113.75ms
step:1098/1750 train_time:124889ms step_avg:113.74ms
step:1099/1750 train_time:124999ms step_avg:113.74ms
step:1100/1750 train_time:125109ms step_avg:113.74ms
step:1101/1750 train_time:125230ms step_avg:113.74ms
step:1102/1750 train_time:125328ms step_avg:113.73ms
step:1103/1750 train_time:125437ms step_avg:113.72ms
step:1104/1750 train_time:125547ms step_avg:113.72ms
step:1105/1750 train_time:125657ms step_avg:113.72ms
step:1106/1750 train_time:125767ms step_avg:113.71ms
step:1107/1750 train_time:125877ms step_avg:113.71ms
step:1108/1750 train_time:126021ms step_avg:113.74ms
step:1109/1750 train_time:126138ms step_avg:113.74ms
step:1110/1750 train_time:126255ms step_avg:113.74ms
step:1111/1750 train_time:126373ms step_avg:113.75ms
step:1112/1750 train_time:126486ms step_avg:113.75ms
step:1113/1750 train_time:126596ms step_avg:113.74ms
step:1114/1750 train_time:126706ms step_avg:113.74ms
step:1115/1750 train_time:126815ms step_avg:113.74ms
step:1116/1750 train_time:126926ms step_avg:113.73ms
step:1117/1750 train_time:127042ms step_avg:113.74ms
step:1118/1750 train_time:127145ms step_avg:113.73ms
step:1119/1750 train_time:127255ms step_avg:113.72ms
step:1120/1750 train_time:127364ms step_avg:113.72ms
step:1121/1750 train_time:127475ms step_avg:113.72ms
step:1122/1750 train_time:127585ms step_avg:113.71ms
step:1123/1750 train_time:127694ms step_avg:113.71ms
step:1124/1750 train_time:127804ms step_avg:113.70ms
step:1125/1750 train_time:127914ms step_avg:113.70ms
step:1125/1750 val_loss:3.4559 train_time:128020ms step_avg:113.80ms
step:1126/1750 train_time:128060ms step_avg:113.73ms
step:1127/1750 train_time:128197ms step_avg:113.75ms
step:1128/1750 train_time:128335ms step_avg:113.77ms
step:1129/1750 train_time:128459ms step_avg:113.78ms
step:1130/1750 train_time:128568ms step_avg:113.78ms
step:1131/1750 train_time:128689ms step_avg:113.78ms
step:1132/1750 train_time:128806ms step_avg:113.79ms
step:1133/1750 train_time:128923ms step_avg:113.79ms
step:1134/1750 train_time:129036ms step_avg:113.79ms
step:1135/1750 train_time:129146ms step_avg:113.79ms
step:1136/1750 train_time:129256ms step_avg:113.78ms
step:1137/1750 train_time:129366ms step_avg:113.78ms
step:1138/1750 train_time:129476ms step_avg:113.77ms
step:1139/1750 train_time:129586ms step_avg:113.77ms
step:1140/1750 train_time:129698ms step_avg:113.77ms
step:1141/1750 train_time:129813ms step_avg:113.77ms
step:1142/1750 train_time:129923ms step_avg:113.77ms
step:1143/1750 train_time:130033ms step_avg:113.76ms
step:1144/1750 train_time:130147ms step_avg:113.77ms
step:1145/1750 train_time:130257ms step_avg:113.76ms
step:1146/1750 train_time:130366ms step_avg:113.76ms
step:1147/1750 train_time:130474ms step_avg:113.75ms
step:1148/1750 train_time:130583ms step_avg:113.75ms
step:1149/1750 train_time:130697ms step_avg:113.75ms
step:1150/1750 train_time:130803ms step_avg:113.74ms
step:1151/1750 train_time:130913ms step_avg:113.74ms
step:1152/1750 train_time:131023ms step_avg:113.73ms
step:1153/1750 train_time:131140ms step_avg:113.74ms
step:1154/1750 train_time:131260ms step_avg:113.74ms
step:1155/1750 train_time:131374ms step_avg:113.74ms
step:1156/1750 train_time:131491ms step_avg:113.75ms
step:1157/1750 train_time:131608ms step_avg:113.75ms
step:1158/1750 train_time:131725ms step_avg:113.75ms
step:1159/1750 train_time:131842ms step_avg:113.75ms
step:1160/1750 train_time:131959ms step_avg:113.76ms
step:1161/1750 train_time:132075ms step_avg:113.76ms
step:1162/1750 train_time:132205ms step_avg:113.77ms
step:1163/1750 train_time:132324ms step_avg:113.78ms
step:1164/1750 train_time:132446ms step_avg:113.79ms
step:1165/1750 train_time:132568ms step_avg:113.79ms
step:1166/1750 train_time:132687ms step_avg:113.80ms
step:1167/1750 train_time:132825ms step_avg:113.82ms
step:1168/1750 train_time:132949ms step_avg:113.83ms
step:1169/1750 train_time:133071ms step_avg:113.83ms
step:1170/1750 train_time:133185ms step_avg:113.83ms
step:1171/1750 train_time:133302ms step_avg:113.84ms
step:1172/1750 train_time:133422ms step_avg:113.84ms
step:1173/1750 train_time:133539ms step_avg:113.84ms
step:1174/1750 train_time:133661ms step_avg:113.85ms
step:1175/1750 train_time:133779ms step_avg:113.85ms
step:1176/1750 train_time:133897ms step_avg:113.86ms
step:1177/1750 train_time:134015ms step_avg:113.86ms
step:1178/1750 train_time:134130ms step_avg:113.86ms
step:1179/1750 train_time:134257ms step_avg:113.87ms
step:1180/1750 train_time:134374ms step_avg:113.88ms
step:1181/1750 train_time:134492ms step_avg:113.88ms
step:1182/1750 train_time:134611ms step_avg:113.88ms
step:1183/1750 train_time:134728ms step_avg:113.89ms
step:1184/1750 train_time:134848ms step_avg:113.89ms
step:1185/1750 train_time:134966ms step_avg:113.90ms
step:1186/1750 train_time:135084ms step_avg:113.90ms
step:1187/1750 train_time:135202ms step_avg:113.90ms
step:1188/1750 train_time:135320ms step_avg:113.91ms
step:1189/1750 train_time:135438ms step_avg:113.91ms
step:1190/1750 train_time:135557ms step_avg:113.91ms
step:1191/1750 train_time:135673ms step_avg:113.92ms
step:1192/1750 train_time:135791ms step_avg:113.92ms
step:1193/1750 train_time:135908ms step_avg:113.92ms
step:1194/1750 train_time:136027ms step_avg:113.93ms
step:1195/1750 train_time:136141ms step_avg:113.93ms
step:1196/1750 train_time:136259ms step_avg:113.93ms
step:1197/1750 train_time:136378ms step_avg:113.93ms
step:1198/1750 train_time:136492ms step_avg:113.93ms
step:1199/1750 train_time:136610ms step_avg:113.94ms
step:1200/1750 train_time:136727ms step_avg:113.94ms
step:1201/1750 train_time:136841ms step_avg:113.94ms
step:1202/1750 train_time:136952ms step_avg:113.94ms
step:1203/1750 train_time:137068ms step_avg:113.94ms
step:1204/1750 train_time:137174ms step_avg:113.93ms
step:1205/1750 train_time:137285ms step_avg:113.93ms
step:1206/1750 train_time:137395ms step_avg:113.93ms
step:1207/1750 train_time:137507ms step_avg:113.92ms
step:1208/1750 train_time:137618ms step_avg:113.92ms
step:1209/1750 train_time:137729ms step_avg:113.92ms
step:1210/1750 train_time:137839ms step_avg:113.92ms
step:1211/1750 train_time:137950ms step_avg:113.91ms
step:1212/1750 train_time:138061ms step_avg:113.91ms
step:1213/1750 train_time:138172ms step_avg:113.91ms
step:1214/1750 train_time:138282ms step_avg:113.91ms
step:1215/1750 train_time:138394ms step_avg:113.90ms
step:1216/1750 train_time:138505ms step_avg:113.90ms
step:1217/1750 train_time:138616ms step_avg:113.90ms
step:1218/1750 train_time:138727ms step_avg:113.90ms
step:1219/1750 train_time:138838ms step_avg:113.90ms
step:1220/1750 train_time:138949ms step_avg:113.89ms
step:1221/1750 train_time:139060ms step_avg:113.89ms
step:1222/1750 train_time:139177ms step_avg:113.89ms
step:1223/1750 train_time:139283ms step_avg:113.89ms
step:1224/1750 train_time:139393ms step_avg:113.88ms
step:1225/1750 train_time:139540ms step_avg:113.91ms
step:1226/1750 train_time:139688ms step_avg:113.94ms
step:1227/1750 train_time:139860ms step_avg:113.99ms
step:1228/1750 train_time:140008ms step_avg:114.01ms
step:1229/1750 train_time:140132ms step_avg:114.02ms
step:1230/1750 train_time:140236ms step_avg:114.01ms
step:1231/1750 train_time:140350ms step_avg:114.01ms
step:1232/1750 train_time:140465ms step_avg:114.01ms
step:1233/1750 train_time:140576ms step_avg:114.01ms
step:1234/1750 train_time:140688ms step_avg:114.01ms
step:1235/1750 train_time:140811ms step_avg:114.02ms
step:1236/1750 train_time:140940ms step_avg:114.03ms
step:1237/1750 train_time:141057ms step_avg:114.03ms
step:1238/1750 train_time:141168ms step_avg:114.03ms
step:1239/1750 train_time:141279ms step_avg:114.03ms
step:1240/1750 train_time:141390ms step_avg:114.02ms
step:1241/1750 train_time:141502ms step_avg:114.02ms
step:1242/1750 train_time:141614ms step_avg:114.02ms
step:1243/1750 train_time:141728ms step_avg:114.02ms
step:1244/1750 train_time:141835ms step_avg:114.02ms
step:1245/1750 train_time:141957ms step_avg:114.02ms
step:1246/1750 train_time:142057ms step_avg:114.01ms
step:1247/1750 train_time:142168ms step_avg:114.01ms
step:1248/1750 train_time:142280ms step_avg:114.01ms
step:1249/1750 train_time:142391ms step_avg:114.00ms
step:1250/1750 train_time:142501ms step_avg:114.00ms
step:1250/1750 val_loss:3.4101 train_time:142608ms step_avg:114.09ms
step:1251/1750 train_time:142661ms step_avg:114.04ms
step:1252/1750 train_time:142818ms step_avg:114.07ms
step:1253/1750 train_time:142936ms step_avg:114.08ms
step:1254/1750 train_time:143054ms step_avg:114.08ms
step:1255/1750 train_time:143173ms step_avg:114.08ms
step:1256/1750 train_time:143291ms step_avg:114.09ms
step:1257/1750 train_time:143402ms step_avg:114.08ms
step:1258/1750 train_time:143513ms step_avg:114.08ms
step:1259/1750 train_time:143630ms step_avg:114.08ms
step:1260/1750 train_time:143734ms step_avg:114.07ms
step:1261/1750 train_time:143845ms step_avg:114.07ms
step:1262/1750 train_time:143956ms step_avg:114.07ms
step:1263/1750 train_time:144066ms step_avg:114.07ms
step:1264/1750 train_time:144208ms step_avg:114.09ms
step:1265/1750 train_time:144291ms step_avg:114.06ms
step:1266/1750 train_time:144401ms step_avg:114.06ms
step:1267/1750 train_time:144512ms step_avg:114.06ms
step:1268/1750 train_time:144623ms step_avg:114.06ms
step:1269/1750 train_time:144734ms step_avg:114.05ms
step:1270/1750 train_time:144845ms step_avg:114.05ms
step:1271/1750 train_time:144958ms step_avg:114.05ms
step:1272/1750 train_time:145068ms step_avg:114.05ms
step:1273/1750 train_time:145178ms step_avg:114.04ms
step:1274/1750 train_time:145295ms step_avg:114.05ms
step:1275/1750 train_time:145402ms step_avg:114.04ms
step:1276/1750 train_time:145511ms step_avg:114.04ms
step:1277/1750 train_time:145622ms step_avg:114.03ms
step:1278/1750 train_time:145735ms step_avg:114.03ms
step:1279/1750 train_time:145844ms step_avg:114.03ms
step:1280/1750 train_time:145955ms step_avg:114.03ms
step:1281/1750 train_time:146067ms step_avg:114.03ms
step:1282/1750 train_time:146180ms step_avg:114.02ms
step:1283/1750 train_time:146288ms step_avg:114.02ms
step:1284/1750 train_time:146420ms step_avg:114.03ms
step:1285/1750 train_time:146534ms step_avg:114.03ms
step:1286/1750 train_time:146645ms step_avg:114.03ms
step:1287/1750 train_time:146755ms step_avg:114.03ms
step:1288/1750 train_time:146865ms step_avg:114.03ms
step:1289/1750 train_time:146976ms step_avg:114.02ms
step:1290/1750 train_time:147087ms step_avg:114.02ms
step:1291/1750 train_time:147198ms step_avg:114.02ms
step:1292/1750 train_time:147309ms step_avg:114.02ms
step:1293/1750 train_time:147421ms step_avg:114.01ms
step:1294/1750 train_time:147533ms step_avg:114.01ms
step:1295/1750 train_time:147644ms step_avg:114.01ms
step:1296/1750 train_time:147754ms step_avg:114.01ms
step:1297/1750 train_time:147865ms step_avg:114.01ms
step:1298/1750 train_time:147976ms step_avg:114.00ms
step:1299/1750 train_time:148090ms step_avg:114.00ms
step:1300/1750 train_time:148197ms step_avg:114.00ms
step:1301/1750 train_time:148308ms step_avg:114.00ms
step:1302/1750 train_time:148419ms step_avg:113.99ms
step:1303/1750 train_time:148530ms step_avg:113.99ms
step:1304/1750 train_time:148661ms step_avg:114.00ms
step:1305/1750 train_time:148753ms step_avg:113.99ms
step:1306/1750 train_time:148863ms step_avg:113.98ms
step:1307/1750 train_time:148974ms step_avg:113.98ms
step:1308/1750 train_time:149085ms step_avg:113.98ms
step:1309/1750 train_time:149197ms step_avg:113.98ms
step:1310/1750 train_time:149309ms step_avg:113.98ms
step:1311/1750 train_time:149420ms step_avg:113.97ms
step:1312/1750 train_time:149531ms step_avg:113.97ms
step:1313/1750 train_time:149643ms step_avg:113.97ms
step:1314/1750 train_time:149754ms step_avg:113.97ms
step:1315/1750 train_time:149865ms step_avg:113.97ms
step:1316/1750 train_time:149976ms step_avg:113.96ms
step:1317/1750 train_time:150101ms step_avg:113.97ms
step:1318/1750 train_time:150197ms step_avg:113.96ms
step:1319/1750 train_time:150309ms step_avg:113.96ms
step:1320/1750 train_time:150421ms step_avg:113.96ms
step:1321/1750 train_time:150533ms step_avg:113.95ms
step:1322/1750 train_time:150645ms step_avg:113.95ms
step:1323/1750 train_time:150757ms step_avg:113.95ms
step:1324/1750 train_time:150868ms step_avg:113.95ms
step:1325/1750 train_time:150980ms step_avg:113.95ms
step:1326/1750 train_time:151092ms step_avg:113.95ms
step:1327/1750 train_time:151203ms step_avg:113.94ms
step:1328/1750 train_time:151315ms step_avg:113.94ms
step:1329/1750 train_time:151424ms step_avg:113.94ms
step:1330/1750 train_time:151535ms step_avg:113.94ms
step:1331/1750 train_time:151646ms step_avg:113.93ms
step:1332/1750 train_time:151757ms step_avg:113.93ms
step:1333/1750 train_time:151869ms step_avg:113.93ms
step:1334/1750 train_time:151980ms step_avg:113.93ms
step:1335/1750 train_time:152091ms step_avg:113.93ms
step:1336/1750 train_time:152203ms step_avg:113.92ms
step:1337/1750 train_time:152315ms step_avg:113.92ms
step:1338/1750 train_time:152425ms step_avg:113.92ms
step:1339/1750 train_time:152536ms step_avg:113.92ms
step:1340/1750 train_time:152646ms step_avg:113.92ms
step:1341/1750 train_time:152758ms step_avg:113.91ms
step:1342/1750 train_time:152869ms step_avg:113.91ms
step:1343/1750 train_time:152980ms step_avg:113.91ms
step:1344/1750 train_time:153091ms step_avg:113.91ms
step:1345/1750 train_time:153202ms step_avg:113.90ms
step:1346/1750 train_time:153314ms step_avg:113.90ms
step:1347/1750 train_time:153432ms step_avg:113.91ms
step:1348/1750 train_time:153555ms step_avg:113.91ms
step:1349/1750 train_time:153665ms step_avg:113.91ms
step:1350/1750 train_time:153795ms step_avg:113.92ms
step:1351/1750 train_time:153900ms step_avg:113.92ms
step:1352/1750 train_time:154015ms step_avg:113.92ms
step:1353/1750 train_time:154129ms step_avg:113.92ms
step:1354/1750 train_time:154240ms step_avg:113.91ms
step:1355/1750 train_time:154351ms step_avg:113.91ms
step:1356/1750 train_time:154462ms step_avg:113.91ms
step:1357/1750 train_time:154572ms step_avg:113.91ms
step:1358/1750 train_time:154684ms step_avg:113.91ms
step:1359/1750 train_time:154794ms step_avg:113.90ms
step:1360/1750 train_time:154912ms step_avg:113.91ms
step:1361/1750 train_time:155019ms step_avg:113.90ms
step:1362/1750 train_time:155127ms step_avg:113.90ms
step:1363/1750 train_time:155239ms step_avg:113.89ms
step:1364/1750 train_time:155350ms step_avg:113.89ms
step:1365/1750 train_time:155461ms step_avg:113.89ms
step:1366/1750 train_time:155572ms step_avg:113.89ms
step:1367/1750 train_time:155683ms step_avg:113.89ms
step:1368/1750 train_time:155794ms step_avg:113.88ms
step:1369/1750 train_time:155904ms step_avg:113.88ms
step:1370/1750 train_time:156022ms step_avg:113.88ms
step:1371/1750 train_time:156126ms step_avg:113.88ms
step:1372/1750 train_time:156236ms step_avg:113.87ms
step:1373/1750 train_time:156347ms step_avg:113.87ms
step:1374/1750 train_time:156458ms step_avg:113.87ms
step:1375/1750 train_time:156570ms step_avg:113.87ms
step:1375/1750 val_loss:3.3700 train_time:156676ms step_avg:113.95ms
step:1376/1750 train_time:156713ms step_avg:113.89ms
step:1377/1750 train_time:156886ms step_avg:113.93ms
step:1378/1750 train_time:157037ms step_avg:113.96ms
step:1379/1750 train_time:157155ms step_avg:113.96ms
step:1380/1750 train_time:157273ms step_avg:113.97ms
step:1381/1750 train_time:157384ms step_avg:113.96ms
step:1382/1750 train_time:157503ms step_avg:113.97ms
step:1383/1750 train_time:157622ms step_avg:113.97ms
step:1384/1750 train_time:157732ms step_avg:113.97ms
step:1385/1750 train_time:157850ms step_avg:113.97ms
step:1386/1750 train_time:157958ms step_avg:113.97ms
step:1387/1750 train_time:158072ms step_avg:113.97ms
step:1388/1750 train_time:158183ms step_avg:113.96ms
step:1389/1750 train_time:158294ms step_avg:113.96ms
step:1390/1750 train_time:158404ms step_avg:113.96ms
step:1391/1750 train_time:158515ms step_avg:113.96ms
step:1392/1750 train_time:158627ms step_avg:113.96ms
step:1393/1750 train_time:158737ms step_avg:113.95ms
step:1394/1750 train_time:158848ms step_avg:113.95ms
step:1395/1750 train_time:158959ms step_avg:113.95ms
step:1396/1750 train_time:159070ms step_avg:113.95ms
step:1397/1750 train_time:159181ms step_avg:113.95ms
step:1398/1750 train_time:159292ms step_avg:113.94ms
step:1399/1750 train_time:159403ms step_avg:113.94ms
step:1400/1750 train_time:159514ms step_avg:113.94ms
step:1401/1750 train_time:159625ms step_avg:113.94ms
step:1402/1750 train_time:159736ms step_avg:113.93ms
step:1403/1750 train_time:159847ms step_avg:113.93ms
step:1404/1750 train_time:159961ms step_avg:113.93ms
step:1405/1750 train_time:160072ms step_avg:113.93ms
step:1406/1750 train_time:160184ms step_avg:113.93ms
step:1407/1750 train_time:160295ms step_avg:113.93ms
step:1408/1750 train_time:160406ms step_avg:113.92ms
step:1409/1750 train_time:160544ms step_avg:113.94ms
step:1410/1750 train_time:160687ms step_avg:113.96ms
step:1411/1750 train_time:160806ms step_avg:113.97ms
step:1412/1750 train_time:160921ms step_avg:113.97ms
step:1413/1750 train_time:161032ms step_avg:113.96ms
step:1414/1750 train_time:161143ms step_avg:113.96ms
step:1415/1750 train_time:161255ms step_avg:113.96ms
step:1416/1750 train_time:161366ms step_avg:113.96ms
step:1417/1750 train_time:161497ms step_avg:113.97ms
step:1418/1750 train_time:161615ms step_avg:113.97ms
step:1419/1750 train_time:161726ms step_avg:113.97ms
step:1420/1750 train_time:161840ms step_avg:113.97ms
step:1421/1750 train_time:161952ms step_avg:113.97ms
step:1422/1750 train_time:162062ms step_avg:113.97ms
step:1423/1750 train_time:162173ms step_avg:113.97ms
step:1424/1750 train_time:162285ms step_avg:113.96ms
step:1425/1750 train_time:162397ms step_avg:113.96ms
step:1426/1750 train_time:162509ms step_avg:113.96ms
step:1427/1750 train_time:162626ms step_avg:113.96ms
step:1428/1750 train_time:162732ms step_avg:113.96ms
step:1429/1750 train_time:162844ms step_avg:113.96ms
step:1430/1750 train_time:162957ms step_avg:113.96ms
step:1431/1750 train_time:163069ms step_avg:113.95ms
step:1432/1750 train_time:163180ms step_avg:113.95ms
step:1433/1750 train_time:163306ms step_avg:113.96ms
step:1434/1750 train_time:163440ms step_avg:113.97ms
step:1435/1750 train_time:163565ms step_avg:113.98ms
step:1436/1750 train_time:163690ms step_avg:113.99ms
step:1437/1750 train_time:163809ms step_avg:113.99ms
step:1438/1750 train_time:163920ms step_avg:113.99ms
step:1439/1750 train_time:164033ms step_avg:113.99ms
step:1440/1750 train_time:164148ms step_avg:113.99ms
step:1441/1750 train_time:164265ms step_avg:113.99ms
step:1442/1750 train_time:164376ms step_avg:113.99ms
step:1443/1750 train_time:164506ms step_avg:114.00ms
step:1444/1750 train_time:164624ms step_avg:114.01ms
step:1445/1750 train_time:164735ms step_avg:114.00ms
step:1446/1750 train_time:164847ms step_avg:114.00ms
step:1447/1750 train_time:164958ms step_avg:114.00ms
step:1448/1750 train_time:165072ms step_avg:114.00ms
step:1449/1750 train_time:165183ms step_avg:114.00ms
step:1450/1750 train_time:165295ms step_avg:114.00ms
step:1451/1750 train_time:165407ms step_avg:114.00ms
step:1452/1750 train_time:165519ms step_avg:113.99ms
step:1453/1750 train_time:165632ms step_avg:113.99ms
step:1454/1750 train_time:165748ms step_avg:113.99ms
step:1455/1750 train_time:165867ms step_avg:114.00ms
step:1456/1750 train_time:165979ms step_avg:114.00ms
step:1457/1750 train_time:166092ms step_avg:114.00ms
step:1458/1750 train_time:166211ms step_avg:114.00ms
step:1459/1750 train_time:166316ms step_avg:113.99ms
step:1460/1750 train_time:166428ms step_avg:113.99ms
step:1461/1750 train_time:166540ms step_avg:113.99ms
step:1462/1750 train_time:166652ms step_avg:113.99ms
step:1463/1750 train_time:166764ms step_avg:113.99ms
step:1464/1750 train_time:166877ms step_avg:113.99ms
step:1465/1750 train_time:166988ms step_avg:113.99ms
step:1466/1750 train_time:167100ms step_avg:113.98ms
step:1467/1750 train_time:167226ms step_avg:113.99ms
step:1468/1750 train_time:167346ms step_avg:114.00ms
step:1469/1750 train_time:167553ms step_avg:114.06ms
step:1470/1750 train_time:167703ms step_avg:114.08ms
step:1471/1750 train_time:167854ms step_avg:114.11ms
step:1472/1750 train_time:167991ms step_avg:114.12ms
step:1473/1750 train_time:168148ms step_avg:114.15ms
step:1474/1750 train_time:168270ms step_avg:114.16ms
step:1475/1750 train_time:168390ms step_avg:114.16ms
step:1476/1750 train_time:168511ms step_avg:114.17ms
step:1477/1750 train_time:168631ms step_avg:114.17ms
step:1478/1750 train_time:168751ms step_avg:114.17ms
step:1479/1750 train_time:168873ms step_avg:114.18ms
step:1480/1750 train_time:168995ms step_avg:114.19ms
step:1481/1750 train_time:169115ms step_avg:114.19ms
step:1482/1750 train_time:169232ms step_avg:114.19ms
step:1483/1750 train_time:169351ms step_avg:114.19ms
step:1484/1750 train_time:169470ms step_avg:114.20ms
step:1485/1750 train_time:169593ms step_avg:114.20ms
step:1486/1750 train_time:169709ms step_avg:114.21ms
step:1487/1750 train_time:169828ms step_avg:114.21ms
step:1488/1750 train_time:169949ms step_avg:114.21ms
step:1489/1750 train_time:170069ms step_avg:114.22ms
step:1490/1750 train_time:170188ms step_avg:114.22ms
step:1491/1750 train_time:170308ms step_avg:114.22ms
step:1492/1750 train_time:170432ms step_avg:114.23ms
step:1493/1750 train_time:170560ms step_avg:114.24ms
step:1494/1750 train_time:170682ms step_avg:114.25ms
step:1495/1750 train_time:170809ms step_avg:114.25ms
step:1496/1750 train_time:170925ms step_avg:114.25ms
step:1497/1750 train_time:171044ms step_avg:114.26ms
step:1498/1750 train_time:171163ms step_avg:114.26ms
step:1499/1750 train_time:171281ms step_avg:114.26ms
step:1500/1750 train_time:171393ms step_avg:114.26ms
step:1500/1750 val_loss:3.3343 train_time:171501ms step_avg:114.33ms
step:1501/1750 train_time:171544ms step_avg:114.29ms
step:1502/1750 train_time:171680ms step_avg:114.30ms
step:1503/1750 train_time:171806ms step_avg:114.31ms
step:1504/1750 train_time:171948ms step_avg:114.33ms
step:1505/1750 train_time:172119ms step_avg:114.36ms
step:1506/1750 train_time:172245ms step_avg:114.37ms
step:1507/1750 train_time:172363ms step_avg:114.37ms
step:1508/1750 train_time:172482ms step_avg:114.38ms
step:1509/1750 train_time:172602ms step_avg:114.38ms
step:1510/1750 train_time:172721ms step_avg:114.39ms
step:1511/1750 train_time:172842ms step_avg:114.39ms
step:1512/1750 train_time:172961ms step_avg:114.39ms
step:1513/1750 train_time:173080ms step_avg:114.40ms
step:1514/1750 train_time:173200ms step_avg:114.40ms
step:1515/1750 train_time:173318ms step_avg:114.40ms
step:1516/1750 train_time:173438ms step_avg:114.41ms
step:1517/1750 train_time:173562ms step_avg:114.41ms
step:1518/1750 train_time:173675ms step_avg:114.41ms
step:1519/1750 train_time:173796ms step_avg:114.41ms
step:1520/1750 train_time:173941ms step_avg:114.43ms
step:1521/1750 train_time:174056ms step_avg:114.44ms
step:1522/1750 train_time:174147ms step_avg:114.42ms
step:1523/1750 train_time:174260ms step_avg:114.42ms
step:1524/1750 train_time:174380ms step_avg:114.42ms
step:1525/1750 train_time:174497ms step_avg:114.42ms
step:1526/1750 train_time:174617ms step_avg:114.43ms
step:1527/1750 train_time:174736ms step_avg:114.43ms
step:1528/1750 train_time:174860ms step_avg:114.44ms
step:1529/1750 train_time:174978ms step_avg:114.44ms
step:1530/1750 train_time:175097ms step_avg:114.44ms
step:1531/1750 train_time:175217ms step_avg:114.45ms
step:1532/1750 train_time:175329ms step_avg:114.44ms
step:1533/1750 train_time:175446ms step_avg:114.45ms
step:1534/1750 train_time:175565ms step_avg:114.45ms
step:1535/1750 train_time:175725ms step_avg:114.48ms
step:1536/1750 train_time:175844ms step_avg:114.48ms
step:1537/1750 train_time:175983ms step_avg:114.50ms
step:1538/1750 train_time:176099ms step_avg:114.50ms
step:1539/1750 train_time:176216ms step_avg:114.50ms
step:1540/1750 train_time:176332ms step_avg:114.50ms
step:1541/1750 train_time:176445ms step_avg:114.50ms
step:1542/1750 train_time:176559ms step_avg:114.50ms
step:1543/1750 train_time:176673ms step_avg:114.50ms
step:1544/1750 train_time:176785ms step_avg:114.50ms
step:1545/1750 train_time:176905ms step_avg:114.50ms
step:1546/1750 train_time:177016ms step_avg:114.50ms
step:1547/1750 train_time:177147ms step_avg:114.51ms
step:1548/1750 train_time:177259ms step_avg:114.51ms
step:1549/1750 train_time:177397ms step_avg:114.52ms
step:1550/1750 train_time:177519ms step_avg:114.53ms
step:1551/1750 train_time:177641ms step_avg:114.53ms
step:1552/1750 train_time:177743ms step_avg:114.52ms
step:1553/1750 train_time:177856ms step_avg:114.52ms
step:1554/1750 train_time:177968ms step_avg:114.52ms
step:1555/1750 train_time:178080ms step_avg:114.52ms
step:1556/1750 train_time:178192ms step_avg:114.52ms
step:1557/1750 train_time:178306ms step_avg:114.52ms
step:1558/1750 train_time:178426ms step_avg:114.52ms
step:1559/1750 train_time:178542ms step_avg:114.52ms
step:1560/1750 train_time:178661ms step_avg:114.53ms
step:1561/1750 train_time:178776ms step_avg:114.53ms
step:1562/1750 train_time:178905ms step_avg:114.54ms
step:1563/1750 train_time:179023ms step_avg:114.54ms
step:1564/1750 train_time:179157ms step_avg:114.55ms
step:1565/1750 train_time:179287ms step_avg:114.56ms
step:1566/1750 train_time:179402ms step_avg:114.56ms
step:1567/1750 train_time:179510ms step_avg:114.56ms
step:1568/1750 train_time:179622ms step_avg:114.55ms
step:1569/1750 train_time:179734ms step_avg:114.55ms
step:1570/1750 train_time:179847ms step_avg:114.55ms
step:1571/1750 train_time:179980ms step_avg:114.56ms
step:1572/1750 train_time:180071ms step_avg:114.55ms
step:1573/1750 train_time:180184ms step_avg:114.55ms
step:1574/1750 train_time:180307ms step_avg:114.55ms
step:1575/1750 train_time:180414ms step_avg:114.55ms
step:1576/1750 train_time:180527ms step_avg:114.55ms
step:1577/1750 train_time:180640ms step_avg:114.55ms
step:1578/1750 train_time:180752ms step_avg:114.55ms
step:1579/1750 train_time:180864ms step_avg:114.54ms
step:1580/1750 train_time:180984ms step_avg:114.55ms
step:1581/1750 train_time:181107ms step_avg:114.55ms
step:1582/1750 train_time:181226ms step_avg:114.55ms
step:1583/1750 train_time:181349ms step_avg:114.56ms
step:1584/1750 train_time:181463ms step_avg:114.56ms
step:1585/1750 train_time:181575ms step_avg:114.56ms
step:1586/1750 train_time:181689ms step_avg:114.56ms
step:1587/1750 train_time:181804ms step_avg:114.56ms
step:1588/1750 train_time:181925ms step_avg:114.56ms
step:1589/1750 train_time:182044ms step_avg:114.57ms
step:1590/1750 train_time:182156ms step_avg:114.56ms
step:1591/1750 train_time:182276ms step_avg:114.57ms
step:1592/1750 train_time:182389ms step_avg:114.57ms
step:1593/1750 train_time:182505ms step_avg:114.57ms
step:1594/1750 train_time:182628ms step_avg:114.57ms
step:1595/1750 train_time:182749ms step_avg:114.58ms
step:1596/1750 train_time:182871ms step_avg:114.58ms
step:1597/1750 train_time:183006ms step_avg:114.59ms
step:1598/1750 train_time:183131ms step_avg:114.60ms
step:1599/1750 train_time:183254ms step_avg:114.61ms
step:1600/1750 train_time:183369ms step_avg:114.61ms
step:1601/1750 train_time:183488ms step_avg:114.61ms
step:1602/1750 train_time:183638ms step_avg:114.63ms
step:1603/1750 train_time:183750ms step_avg:114.63ms
step:1604/1750 train_time:183869ms step_avg:114.63ms
step:1605/1750 train_time:183988ms step_avg:114.63ms
step:1606/1750 train_time:184107ms step_avg:114.64ms
step:1607/1750 train_time:184226ms step_avg:114.64ms
step:1608/1750 train_time:184345ms step_avg:114.64ms
step:1609/1750 train_time:184461ms step_avg:114.64ms
step:1610/1750 train_time:184581ms step_avg:114.65ms
step:1611/1750 train_time:184702ms step_avg:114.65ms
step:1612/1750 train_time:184822ms step_avg:114.65ms
step:1613/1750 train_time:184941ms step_avg:114.66ms
step:1614/1750 train_time:185059ms step_avg:114.66ms
step:1615/1750 train_time:185174ms step_avg:114.66ms
step:1616/1750 train_time:185286ms step_avg:114.66ms
step:1617/1750 train_time:185399ms step_avg:114.66ms
step:1618/1750 train_time:185511ms step_avg:114.65ms
step:1619/1750 train_time:185623ms step_avg:114.65ms
step:1620/1750 train_time:185736ms step_avg:114.65ms
step:1621/1750 train_time:185848ms step_avg:114.65ms
step:1622/1750 train_time:185960ms step_avg:114.65ms
step:1623/1750 train_time:186072ms step_avg:114.65ms
step:1624/1750 train_time:186185ms step_avg:114.65ms
step:1625/1750 train_time:186298ms step_avg:114.64ms
step:1625/1750 val_loss:3.3038 train_time:186407ms step_avg:114.71ms
step:1626/1750 train_time:186443ms step_avg:114.66ms
step:1627/1750 train_time:186607ms step_avg:114.69ms
step:1628/1750 train_time:186735ms step_avg:114.70ms
step:1629/1750 train_time:186865ms step_avg:114.71ms
step:1630/1750 train_time:187013ms step_avg:114.73ms
step:1631/1750 train_time:187134ms step_avg:114.74ms
step:1632/1750 train_time:187253ms step_avg:114.74ms
step:1633/1750 train_time:187371ms step_avg:114.74ms
step:1634/1750 train_time:187491ms step_avg:114.74ms
step:1635/1750 train_time:187610ms step_avg:114.75ms
step:1636/1750 train_time:187737ms step_avg:114.75ms
step:1637/1750 train_time:187856ms step_avg:114.76ms
step:1638/1750 train_time:187975ms step_avg:114.76ms
step:1639/1750 train_time:188093ms step_avg:114.76ms
step:1640/1750 train_time:188227ms step_avg:114.77ms
step:1641/1750 train_time:188326ms step_avg:114.76ms
step:1642/1750 train_time:188437ms step_avg:114.76ms
step:1643/1750 train_time:188549ms step_avg:114.76ms
step:1644/1750 train_time:188662ms step_avg:114.76ms
step:1645/1750 train_time:188774ms step_avg:114.76ms
step:1646/1750 train_time:188887ms step_avg:114.75ms
step:1647/1750 train_time:189045ms step_avg:114.78ms
step:1648/1750 train_time:189117ms step_avg:114.76ms
step:1649/1750 train_time:189225ms step_avg:114.75ms
step:1650/1750 train_time:189337ms step_avg:114.75ms
step:1651/1750 train_time:189449ms step_avg:114.75ms
step:1652/1750 train_time:189566ms step_avg:114.75ms
step:1653/1750 train_time:189674ms step_avg:114.75ms
step:1654/1750 train_time:189786ms step_avg:114.74ms
step:1655/1750 train_time:189911ms step_avg:114.75ms
step:1656/1750 train_time:190018ms step_avg:114.75ms
step:1657/1750 train_time:190122ms step_avg:114.74ms
step:1658/1750 train_time:190234ms step_avg:114.74ms
step:1659/1750 train_time:190351ms step_avg:114.74ms
step:1660/1750 train_time:190462ms step_avg:114.74ms
step:1661/1750 train_time:190575ms step_avg:114.74ms
step:1662/1750 train_time:190689ms step_avg:114.73ms
step:1663/1750 train_time:190815ms step_avg:114.74ms
step:1664/1750 train_time:190949ms step_avg:114.75ms
step:1665/1750 train_time:191051ms step_avg:114.75ms
step:1666/1750 train_time:191158ms step_avg:114.74ms
step:1667/1750 train_time:191270ms step_avg:114.74ms
step:1668/1750 train_time:191383ms step_avg:114.74ms
step:1669/1750 train_time:191497ms step_avg:114.74ms
step:1670/1750 train_time:191609ms step_avg:114.74ms
step:1671/1750 train_time:191721ms step_avg:114.73ms
step:1672/1750 train_time:191838ms step_avg:114.74ms
step:1673/1750 train_time:191953ms step_avg:114.74ms
step:1674/1750 train_time:192065ms step_avg:114.73ms
step:1675/1750 train_time:192182ms step_avg:114.74ms
step:1676/1750 train_time:192304ms step_avg:114.74ms
step:1677/1750 train_time:192423ms step_avg:114.74ms
step:1678/1750 train_time:192542ms step_avg:114.75ms
step:1679/1750 train_time:192655ms step_avg:114.74ms
step:1680/1750 train_time:192767ms step_avg:114.74ms
step:1681/1750 train_time:192880ms step_avg:114.74ms
step:1682/1750 train_time:192994ms step_avg:114.74ms
step:1683/1750 train_time:193106ms step_avg:114.74ms
step:1684/1750 train_time:193218ms step_avg:114.74ms
step:1685/1750 train_time:193332ms step_avg:114.74ms
step:1686/1750 train_time:193443ms step_avg:114.73ms
step:1687/1750 train_time:193566ms step_avg:114.74ms
step:1688/1750 train_time:193679ms step_avg:114.74ms
step:1689/1750 train_time:193792ms step_avg:114.74ms
step:1690/1750 train_time:193905ms step_avg:114.74ms
step:1691/1750 train_time:194021ms step_avg:114.74ms
step:1692/1750 train_time:194129ms step_avg:114.73ms
step:1693/1750 train_time:194242ms step_avg:114.73ms
step:1694/1750 train_time:194357ms step_avg:114.73ms
step:1695/1750 train_time:194470ms step_avg:114.73ms
step:1696/1750 train_time:194584ms step_avg:114.73ms
step:1697/1750 train_time:194701ms step_avg:114.73ms
step:1698/1750 train_time:194813ms step_avg:114.73ms
step:1699/1750 train_time:194926ms step_avg:114.73ms
step:1700/1750 train_time:195039ms step_avg:114.73ms
step:1701/1750 train_time:195152ms step_avg:114.73ms
step:1702/1750 train_time:195270ms step_avg:114.73ms
step:1703/1750 train_time:195403ms step_avg:114.74ms
step:1704/1750 train_time:195519ms step_avg:114.74ms
step:1705/1750 train_time:195643ms step_avg:114.75ms
step:1706/1750 train_time:195746ms step_avg:114.74ms
step:1707/1750 train_time:195858ms step_avg:114.74ms
step:1708/1750 train_time:195971ms step_avg:114.74ms
step:1709/1750 train_time:196085ms step_avg:114.74ms
step:1710/1750 train_time:196198ms step_avg:114.74ms
step:1711/1750 train_time:196314ms step_avg:114.74ms
step:1712/1750 train_time:196427ms step_avg:114.74ms
step:1713/1750 train_time:196641ms step_avg:114.79ms
step:1714/1750 train_time:196788ms step_avg:114.81ms
step:1715/1750 train_time:196916ms step_avg:114.82ms
step:1716/1750 train_time:197035ms step_avg:114.82ms
step:1717/1750 train_time:197162ms step_avg:114.83ms
step:1718/1750 train_time:197282ms step_avg:114.83ms
step:1719/1750 train_time:197405ms step_avg:114.84ms
step:1720/1750 train_time:197525ms step_avg:114.84ms
step:1721/1750 train_time:197647ms step_avg:114.84ms
step:1722/1750 train_time:197765ms step_avg:114.85ms
step:1723/1750 train_time:197887ms step_avg:114.85ms
step:1724/1750 train_time:198006ms step_avg:114.85ms
step:1725/1750 train_time:198128ms step_avg:114.86ms
step:1726/1750 train_time:198248ms step_avg:114.86ms
step:1727/1750 train_time:198388ms step_avg:114.87ms
step:1728/1750 train_time:198488ms step_avg:114.87ms
step:1729/1750 train_time:198609ms step_avg:114.87ms
step:1730/1750 train_time:198728ms step_avg:114.87ms
step:1731/1750 train_time:198849ms step_avg:114.88ms
step:1732/1750 train_time:198969ms step_avg:114.88ms
step:1733/1750 train_time:199089ms step_avg:114.88ms
step:1734/1750 train_time:199210ms step_avg:114.88ms
step:1735/1750 train_time:199330ms step_avg:114.89ms
step:1736/1750 train_time:199450ms step_avg:114.89ms
step:1737/1750 train_time:199570ms step_avg:114.89ms
step:1738/1750 train_time:199690ms step_avg:114.90ms
step:1739/1750 train_time:199812ms step_avg:114.90ms
step:1740/1750 train_time:199931ms step_avg:114.90ms
step:1741/1750 train_time:200055ms step_avg:114.91ms
step:1742/1750 train_time:200175ms step_avg:114.91ms
step:1743/1750 train_time:200297ms step_avg:114.92ms
step:1744/1750 train_time:200417ms step_avg:114.92ms
step:1745/1750 train_time:200534ms step_avg:114.92ms
step:1746/1750 train_time:200650ms step_avg:114.92ms
step:1747/1750 train_time:200770ms step_avg:114.92ms
step:1748/1750 train_time:200891ms step_avg:114.93ms
step:1749/1750 train_time:201003ms step_avg:114.92ms
step:1750/1750 train_time:201118ms step_avg:114.92ms
step:1750/1750 val_loss:3.2800 train_time:201227ms step_avg:114.99ms
peak memory allocated: 33278 MiB reserved: 49112 MiB
